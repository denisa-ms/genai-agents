{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from openai import AzureOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchFieldDataType,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    SearchIndex,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters\n",
    ")\n",
    "\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "# Configure environment variables\n",
    "service_endpoint = os.getenv(\"COPILOT_DEMO_AISEARCH_ENDPOINT\")\n",
    "index_name = os.getenv(\"COPILOT_DEMO_AISEARCH_INDEX\")\n",
    "key = os.getenv(\"COPILOT_DEMO_AISEARCH_ADMIN_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPILOT_DEMO_AZURE_OPENAI_ENDPOINT = os.getenv(\"COPILOT_DEMO_AZURE_OPENAI_ENDPOINT\")\n",
    "COPILOT_DEMO_EMBEDDINGS_ADA_DEPLOYMENT_NAME =\"text-embedding-ada-002\"\n",
    "COPILOT_DEMO_AZURE_OPENAI_API_VERSION=os.getenv(\"COPILOT_DEMO_AZURE_OPENAI_API_VERSION\")\n",
    "COPILOT_DEMO_AZURE_OPENAI_API_KEY = os.getenv(\"COPILOT_DEMO_AZURE_OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI API\n",
    "aoai_client = AzureOpenAI(\n",
    "  azure_endpoint = COPILOT_DEMO_AZURE_OPENAI_ENDPOINT,\n",
    "  api_key = COPILOT_DEMO_AZURE_OPENAI_API_KEY,\n",
    "  api_version = COPILOT_DEMO_AZURE_OPENAI_API_VERSION,\n",
    ")\n",
    "credential = AzureKeyCredential(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Document Embeddings using OpenAI Ada Model\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "# Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "def calc_embeddings(text):\n",
    "    # model = \"deployment_name\"\n",
    "    embeddings = aoai_client.embeddings.create(input = [text], model=COPILOT_DEMO_EMBEDDINGS_ADA_DEPLOYMENT_NAME).data[0].embedding\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into 1000 char long chunks with 30 char overlap\n",
    "# split [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=30,\n",
    ")\n",
    "\n",
    "documentName = \"moby dick book\"\n",
    "fileName = \"./data/moby dick.pdf\"\n",
    "loader = PyPDFLoader(fileName)\n",
    "pages = loader.load_and_split(text_splitter=splitter)\n",
    "print(\"Number of pages: \", len(pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import uuid\n",
    "df = pd.DataFrame(columns=['id','document_name', 'content', 'embedding'])\n",
    "for page in pages:\n",
    "    df.loc[len(df.index)] = [str(uuid.uuid4()), documentName, page.page_content, \"\"]  \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the embeddings using openAI ada \n",
    "df[\"embedding\"] = df.content.apply(lambda x: calc_embeddings(x))\n",
    "df.to_csv('./data/aia_embeddings.csv', index=False)\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output embeddings to json file\n",
    "output_path = os.path.join('..', 'data', 'moby_dick_with_embeddings.json')\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    df.to_json(f, orient='records', default_handler=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books created\n"
     ]
    }
   ],
   "source": [
    "# Create a search index\n",
    "index_client = SearchIndexClient(endpoint=service_endpoint, credential=credential)\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"document_name\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"embedding\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"my-vector-config\")\n",
    "]\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "        profiles=[VectorSearchProfile(name=\"my-vector-config\", algorithm_configuration_name=\"my-algorithms-config\")],\n",
    "        algorithms=[HnswAlgorithmConfiguration(name=\"my-algorithms-config\")],\n",
    "    )\n",
    "\n",
    "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f'{result.name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 1446 pages\n"
     ]
    }
   ],
   "source": [
    "# option 1: upload documents to the index\n",
    "from azure.search.documents import SearchClient\n",
    "import json\n",
    "\n",
    "# Upload some documents to the index\n",
    "output_path = os.path.join('.', 'data', 'moby_dick_with_embeddings.json')\n",
    "with open(output_path, 'r') as file:  \n",
    "    documents = json.load(file)  \n",
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "result = search_client.upload_documents(documents)\n",
    "print(f\"Uploaded {len(documents)} pages\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
