<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3A%22LLM%22%20OR%20all%3A%22Large%20Language%20Model%22%26id_list%3D%26start%3D0%26max_results%3D100" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:"LLM" OR all:"Large Language Model"&amp;id_list=&amp;start=0&amp;max_results=100</title>
  <id>http://arxiv.org/api/ZmOAyc3p4fyBv95QRqJ2rWJsd/w</id>
  <updated>2025-01-25T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">29766</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">100</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2501.13927v1</id>
    <updated>2025-01-23T18:59:47Z</updated>
    <published>2025-01-23T18:59:47Z</published>
    <title>CRPO: Confidence-Reward Driven Preference Optimization for Machine
  Translation</title>
    <summary>  Large language models (LLMs) have shown great potential in natural language
processing tasks, but their application to machine translation (MT) remains
challenging due to pretraining on English-centric data and the complexity of
reinforcement learning from human feedback (RLHF). Direct Preference
Optimization (DPO) has emerged as a simpler and more efficient alternative, but
its performance depends heavily on the quality of preference data. To address
this, we propose Confidence-Reward driven Preference Optimization (CRPO), a
novel method that combines reward scores with model confidence to improve data
selection for fine-tuning. CRPO selects challenging sentence pairs where the
model is uncertain or underperforms, leading to more effective learning. While
primarily designed for LLMs, CRPO also generalizes to encoder-decoder models
like NLLB, demonstrating its versatility. Empirical results show that CRPO
outperforms existing methods such as RS-DPO, RSO and MBR score in both
translation accuracy and data efficiency.
</summary>
    <author>
      <name>Guofeng Cui</name>
    </author>
    <author>
      <name>Pichao Wang</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Zemian Ke</name>
    </author>
    <author>
      <name>Zhu Liu</name>
    </author>
    <author>
      <name>Vimal Bhat</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13921v1</id>
    <updated>2025-01-23T18:59:02Z</updated>
    <published>2025-01-23T18:59:02Z</published>
    <title>The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama
  with Vision-Aware and Function-Calling Capabilities</title>
    <summary>  Breeze 2 is a suite of advanced multi-modal language models, available in 3B
and 8B parameter configurations, specifically designed to enhance Traditional
Chinese language representation. Building upon the Llama 3, Breeze 2 continues
pretraining on an extensive corpus to enhance the linguistic and cultural
heritage of Traditional Chinese. It incorporates vision-aware capabilities
through a visual encoder and a bridge module, and supports function-calling via
prompt templates and post-training on function-calling data. The effectiveness
of Breeze 2 is benchmarked across various tasks, including Taiwan general
knowledge, instruction-following, long context, function calling, and vision
understanding. Furthermore, we showcase the capabilities of the its 3B model in
a mobile application. We are publicly releasing all Breeze 2 models under the
Llama 3 Community License.
</summary>
    <author>
      <name>Chan-Jan Hsu</name>
    </author>
    <author>
      <name>Chia-Sheng Liu</name>
    </author>
    <author>
      <name>Meng-Hsi Chen</name>
    </author>
    <author>
      <name>Muxi Chen</name>
    </author>
    <author>
      <name>Po-Chun Hsu</name>
    </author>
    <author>
      <name>Yi-Chang Chen</name>
    </author>
    <author>
      <name>Da-Shan Shiu</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13912v1</id>
    <updated>2025-01-23T18:49:33Z</updated>
    <published>2025-01-23T18:49:33Z</published>
    <title>Analysis of Indic Language Capabilities in LLMs</title>
    <summary>  This report evaluates the performance of text-in text-out Large Language
Models (LLMs) to understand and generate Indic languages. This evaluation is
used to identify and prioritize Indic languages suited for inclusion in safety
benchmarks. We conduct this study by reviewing existing evaluation studies and
datasets; and a set of twenty-eight LLMs that support Indic languages. We
analyze the LLMs on the basis of the training data, license for model and data,
type of access and model developers. We also compare Indic language performance
across evaluation datasets and find that significant performance disparities in
performance across Indic languages. Hindi is the most widely represented
language in models. While model performance roughly correlates with number of
speakers for the top five languages, the assessment after that varies.
</summary>
    <author>
      <name>Aatman Vaidya</name>
    </author>
    <author>
      <name>Tarunima Prabhakar</name>
    </author>
    <author>
      <name>Denny George</name>
    </author>
    <author>
      <name>Swair Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 2 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13904v1</id>
    <updated>2025-01-23T18:34:09Z</updated>
    <published>2025-01-23T18:34:09Z</published>
    <title>Privacy-Preserving Personalized Federated Prompt Learning for Multimodal
  Large Language Models</title>
    <summary>  Multimodal Large Language Models (LLMs) are pivotal in revolutionizing
customer support and operations by integrating multiple modalities such as
text, images, and audio. Federated Prompt Learning (FPL) is a recently proposed
approach that combines pre-trained multimodal LLMs such as vision-language
models with federated learning to create personalized, privacy-preserving AI
systems. However, balancing the competing goals of personalization,
generalization, and privacy remains a significant challenge.
Over-personalization can lead to overfitting, reducing generalizability, while
stringent privacy measures, such as differential privacy, can hinder both
personalization and generalization. In this paper, we propose a Differentially
Private Federated Prompt Learning (DP-FPL) approach to tackle this challenge by
leveraging a low-rank adaptation scheme to capture generalization while
maintaining a residual term that preserves expressiveness for personalization.
To ensure privacy, we introduce a novel method where we apply local
differential privacy to the two low-rank components of the local prompt, and
global differential privacy to the global prompt. Our approach mitigates the
impact of privacy noise on the model performance while balancing the tradeoff
between personalization and generalization. Extensive experiments demonstrate
the effectiveness of our approach over other benchmarks.
</summary>
    <author>
      <name>Linh Tran</name>
    </author>
    <author>
      <name>Wei Sun</name>
    </author>
    <author>
      <name>Stacy Patterson</name>
    </author>
    <author>
      <name>Ana Milanova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICLR 2025 main conference track</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13884v1</id>
    <updated>2025-01-23T17:57:18Z</updated>
    <published>2025-01-23T17:57:18Z</published>
    <title>Exploring Finetuned Audio-LLM on Heart Murmur Features</title>
    <summary>  Large language models (LLMs) for audio have excelled in recognizing and
analyzing human speech, music, and environmental sounds. However, their
potential for understanding other types of sounds, particularly biomedical
sounds, remains largely underexplored despite significant scientific interest.
In this study, we focus on diagnosing cardiovascular diseases using
phonocardiograms, i.e., heart sounds. Most existing deep neural network (DNN)
paradigms are restricted to heart murmur classification (healthy vs unhealthy)
and do not predict other acoustic features of the murmur such as timing,
grading, harshness, pitch, and quality, which are important in helping
physicians diagnose the underlying heart conditions. We propose to finetune an
audio LLM, Qwen2-Audio, on the PhysioNet CirCor DigiScope phonocardiogram (PCG)
dataset and evaluate its performance in classifying 11 expert-labeled murmur
features. Additionally, we aim to achieve more noise-robust and generalizable
system by exploring a preprocessing segmentation algorithm using an audio
representation model, SSAMBA. Our results indicate that the LLM-based model
outperforms state-of-the-art methods in 8 of the 11 features and performs
comparably in the remaining 3. Moreover, the LLM successfully classifies
long-tail murmur features with limited training data, a task that all previous
methods have failed to classify. These findings underscore the potential of
audio LLMs as assistants to human cardiologists in enhancing heart disease
diagnosis.
</summary>
    <author>
      <name>Adrian Florea</name>
    </author>
    <author>
      <name>Xilin Jiang</name>
    </author>
    <author>
      <name>Nima Mesgarani</name>
    </author>
    <author>
      <name>Xiaofan Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, and 3 tables. Submitted to IEEE/ACM Conference on
  Connected Health: Applications, Systems , and Engineering Technologies</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13881v1</id>
    <updated>2025-01-23T17:56:07Z</updated>
    <published>2025-01-23T17:56:07Z</published>
    <title>The machine learning platform for developers of large systems</title>
    <summary>  The machine learning system in the form of Retrieval Augmented Generation
(RAG) has developed steadily since about 2021. RAG could be observed as a
version of the knowledge transfer. In the studied case, the large computing
systems are observed as the application point of RAG, which includes large
language model (LLM), as a partner for the developing team. Such an approach
has advantages during the development process and further in exploitation time.
</summary>
    <author>
      <name>Alexey Naikov</name>
    </author>
    <author>
      <name>Anatoly Oreshkin</name>
    </author>
    <author>
      <name>Alexey Shvetsov</name>
    </author>
    <author>
      <name>Andrey Shevel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13880v1</id>
    <updated>2025-01-23T17:54:19Z</updated>
    <published>2025-01-23T17:54:19Z</published>
    <title>A RAG-Based Institutional Assistant</title>
    <summary>  Although large language models (LLMs) demonstrate strong text generation
capabilities, they struggle in scenarios requiring access to structured
knowledge bases or specific documents, limiting their effectiveness in
knowledge-intensive tasks. To address this limitation, retrieval-augmented
generation (RAG) models have been developed, enabling generative models to
incorporate relevant document fragments into their inputs. In this paper, we
design and evaluate a RAG-based virtual assistant specifically tailored for the
University of S\~ao Paulo. Our system architecture comprises two key modules: a
retriever and a generative model. We experiment with different types of models
for both components, adjusting hyperparameters such as chunk size and the
number of retrieved documents. Our optimal retriever model achieves a Top-5
accuracy of 30%, while our most effective generative model scores 22.04\%
against ground truth answers. Notably, when the correct document chunks are
supplied to the LLMs, accuracy significantly improves to 54.02%, an increase of
over 30 percentage points. Conversely, without contextual input, performance
declines to 13.68%. These findings highlight the critical role of database
access in enhancing LLM performance. They also reveal the limitations of
current semantic search methods in accurately identifying relevant documents
and underscore the ongoing challenges LLMs face in generating precise
responses.
</summary>
    <author>
      <name>Gustavo Kuratomi</name>
    </author>
    <author>
      <name>Paulo Pirozelli</name>
    </author>
    <author>
      <name>Fabio G. Cozman</name>
    </author>
    <author>
      <name>Sarajane M. Peres</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13833v1</id>
    <updated>2025-01-23T16:58:18Z</updated>
    <published>2025-01-23T16:58:18Z</published>
    <title>On the Reasoning Capacity of AI Models and How to Quantify It</title>
    <summary>  Recent advances in Large Language Models (LLMs) have intensified the debate
surrounding the fundamental nature of their reasoning capabilities. While
achieving high performance on benchmarks such as GPQA and MMLU, these models
exhibit limitations in more complex reasoning tasks, highlighting the need for
more rigorous evaluation methodologies. We propose a novel phenomenological
approach that goes beyond traditional accuracy metrics to probe the underlying
mechanisms of model behavior, establishing a framework that could broadly
impact how we analyze and understand AI systems. Using positional bias in
multiple-choice reasoning tasks as a case study, we demonstrate how systematic
perturbations can reveal fundamental aspects of model decision-making. To
analyze these behaviors, we develop two complementary phenomenological models:
a Probabilistic Mixture Model (PMM) that decomposes model responses into
reasoning, memorization, and guessing components and an Information-Theoretic
Consistency (ITC) analysis that quantifies the relationship between model
confidence and strategy selection. Through controlled experiments on reasoning
benchmarks, we show that true reasoning remains challenging for current models,
with apparent success often relying on sophisticated combinations of
memorization and pattern matching rather than genuine logical deduction. More
fundamentally, we demonstrate that accuracy alone often overstates a model's
reasoning abilities, as model behavior can be characterized through underlying
mechanisms in the phase space of cognitive strategies, revealing how models
dynamically balance different approaches when responding to queries. This
framework enables quantitative criteria for real-world deployments, allowing
applications to specify reliability thresholds based on strategy distributions
rather than aggregate performance metrics.
</summary>
    <author>
      <name>Santosh Kumar Radha</name>
    </author>
    <author>
      <name>Oktay Goktas</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13831v1</id>
    <updated>2025-01-23T16:54:27Z</updated>
    <published>2025-01-23T16:54:27Z</published>
    <title>Predicting Compact Phrasal Rewrites with Large Language Models for ASR
  Post Editing</title>
    <summary>  Large Language Models (LLMs) excel at rewriting tasks such as text style
transfer and grammatical error correction. While there is considerable overlap
between the inputs and outputs in these tasks, the decoding cost still
increases with output length, regardless of the amount of overlap. By
leveraging the overlap between the input and the output, Kaneko and Okazaki
(2023) proposed model-agnostic edit span representations to compress the
rewrites to save computation. They reported an output length reduction rate of
nearly 80% with minimal accuracy impact in four rewriting tasks. In this paper,
we propose alternative edit phrase representations inspired by phrase-based
statistical machine translation. We systematically compare our phrasal
representations with their span representations. We apply the LLM rewriting
model to the task of Automatic Speech Recognition (ASR) post editing and show
that our target-phrase-only edit representation has the best
efficiency-accuracy trade-off. On the LibriSpeech test set, our method closes
50-60% of the WER gap between the edit span model and the full rewrite model
while losing only 10-20% of the length reduction rate of the edit span model.
</summary>
    <author>
      <name>Hao Zhang</name>
    </author>
    <author>
      <name>Felix Stahlberg</name>
    </author>
    <author>
      <name>Shankar Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by ICASSP 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13824v1</id>
    <updated>2025-01-23T16:45:51Z</updated>
    <published>2025-01-23T16:45:51Z</published>
    <title>Hallucinations Can Improve Large Language Models in Drug Discovery</title>
    <summary>  Concerns about hallucinations in Large Language Models (LLMs) have been
raised by researchers, yet their potential in areas where creativity is vital,
such as drug discovery, merits exploration. In this paper, we come up with the
hypothesis that hallucinations can improve LLMs in drug discovery. To verify
this hypothesis, we use LLMs to describe the SMILES string of molecules in
natural language and then incorporate these descriptions as part of the prompt
to address specific tasks in drug discovery. Evaluated on seven LLMs and five
classification tasks, our findings confirm the hypothesis: LLMs can achieve
better performance with text containing hallucinations. Notably, Llama-3.1-8B
achieves an 18.35% gain in ROC-AUC compared to the baseline without
hallucination. Furthermore, hallucinations generated by GPT-4o provide the most
consistent improvements across models. Additionally, we conduct empirical
analyses and a case study to investigate key factors affecting performance and
the underlying reasons. Our research sheds light on the potential use of
hallucinations for LLMs and offers new perspectives for future research
leveraging LLMs in drug discovery.
</summary>
    <author>
      <name>Shuzhou Yuan</name>
    </author>
    <author>
      <name>Michael Färber</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13824v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13824v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13816v1</id>
    <updated>2025-01-23T16:37:44Z</updated>
    <published>2025-01-23T16:37:44Z</published>
    <title>Large Language Model driven Policy Exploration for Recommender Systems</title>
    <summary>  Recent advancements in Recommender Systems (RS) have incorporated
Reinforcement Learning (RL), framing the recommendation as a Markov Decision
Process (MDP). However, offline RL policies trained on static user data are
vulnerable to distribution shift when deployed in dynamic online environments.
Additionally, excessive focus on exploiting short-term relevant items can
hinder exploration, leading to suboptimal recommendations and negatively
impacting long-term user gains. Online RL-based RS also face challenges in
production deployment, due to the risks of exposing users to untrained or
unstable policies. Large Language Models (LLMs) offer a promising solution to
mimic user objectives and preferences for pre-training policies offline to
enhance the initial recommendations in online settings. Effectively managing
distribution shift and balancing exploration are crucial for improving RL-based
RS, especially when leveraging LLM-based pre-training. To address these
challenges, we propose an Interaction-Augmented Learned Policy (iALP) that
utilizes user preferences distilled from an LLM. Our approach involves
prompting the LLM with user states to extract item preferences, learning
rewards based on feedback, and updating the RL policy using an actor-critic
framework. Furthermore, to deploy iALP in an online scenario, we introduce an
adaptive variant, A-iALP, that implements a simple fine-tuning strategy
(A-iALP$_{ft}$), and an adaptive approach (A-iALP$_{ap}$) designed to mitigate
issues with compromised policies and limited exploration. Experiments across
three simulated environments demonstrate that A-iALP introduces substantial
performance improvements
</summary>
    <author>
      <name>Jie Wang</name>
    </author>
    <author>
      <name>Alexandros Karatzoglou</name>
    </author>
    <author>
      <name>Ioannis Arapakis</name>
    </author>
    <author>
      <name>Joemon M. Jose</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13802v1</id>
    <updated>2025-01-23T16:21:15Z</updated>
    <published>2025-01-23T16:21:15Z</published>
    <title>Enhancing LLMs for Governance with Human Oversight: Evaluating and
  Aligning LLMs on Expert Classification of Climate Misinformation for
  Detecting False or Misleading Claims about Climate Change</title>
    <summary>  Climate misinformation is a problem that has the potential to be
substantially aggravated by the development of Large Language Models (LLMs). In
this study we evaluate the potential for LLMs to be part of the solution for
mitigating online dis/misinformation rather than the problem. Employing a
public expert annotated dataset and a curated sample of social media content we
evaluate the performance of proprietary vs. open source LLMs on climate
misinformation classification task, comparing them to existing climate-focused
computer-assisted tools and expert assessments. Results show (1)
state-of-the-art (SOTA) open-source models substantially under-perform in
classifying climate misinformation compared to proprietary models, (2) existing
climate-focused computer-assisted tools leveraging expert-annotated datasets
continues to outperform many of proprietary models, including GPT-4o, and (3)
demonstrate the efficacy and generalizability of fine-tuning GPT-3.5-turbo on
expert annotated dataset in classifying claims about climate change at the
equivalency of climate change experts with over 20 years of experience in
climate communication. These findings highlight 1) the importance of
incorporating human-oversight, such as incorporating expert-annotated datasets
in training LLMs, for governance tasks that require subject-matter expertise
like classifying climate misinformation, and 2) the potential for LLMs in
facilitating civil society organizations to engage in various governance tasks
such as classifying false or misleading claims in domains beyond climate change
such as politics and health science.
</summary>
    <author>
      <name>Mowafak Allaham</name>
    </author>
    <author>
      <name>Ayse D. Lokmanoglu</name>
    </author>
    <author>
      <name>Sol P. Hart</name>
    </author>
    <author>
      <name>Erik C. Nisbet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the AI Governance Workshop at AAAI 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13779v1</id>
    <updated>2025-01-23T15:58:14Z</updated>
    <published>2025-01-23T15:58:14Z</published>
    <title>Not Every AI Problem is a Data Problem: We Should Be Intentional About
  Data Scaling</title>
    <summary>  While Large Language Models require more and more data to train and scale,
rather than looking for any data to acquire, we should consider what types of
tasks are more likely to benefit from data scaling. We should be intentional in
our data acquisition. We argue that the topology of data itself informs which
tasks to prioritize in data scaling, and shapes the development of the next
generation of compute paradigms for tasks where data scaling is inefficient, or
even insufficient.
</summary>
    <author>
      <name>Tanya Rodchenko</name>
    </author>
    <author>
      <name>Natasha Noy</name>
    </author>
    <author>
      <name>Nino Scherrer</name>
    </author>
    <author>
      <name>Jennifer Prendki</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13778v1</id>
    <updated>2025-01-23T15:55:07Z</updated>
    <published>2025-01-23T15:55:07Z</published>
    <title>Explainable XR: Understanding User Behaviors of XR Environments using
  LLM-assisted Analytics Framework</title>
    <summary>  We present Explainable XR, an end-to-end framework for analyzing user
behavior in diverse eXtended Reality (XR) environments by leveraging Large
Language Models (LLMs) for data interpretation assistance. Existing XR user
analytics frameworks face challenges in handling cross-virtuality - AR, VR, MR
- transitions, multi-user collaborative application scenarios, and the
complexity of multimodal data. Explainable XR addresses these challenges by
providing a virtuality-agnostic solution for the collection, analysis, and
visualization of immersive sessions. We propose three main components in our
framework: (1) A novel user data recording schema, called User Action
Descriptor (UAD), that can capture the users' multimodal actions, along with
their intents and the contexts; (2) a platform-agnostic XR session recorder,
and (3) a visual analytics interface that offers LLM-assisted insights tailored
to the analysts' perspectives, facilitating the exploration and analysis of the
recorded XR session data. We demonstrate the versatility of Explainable XR by
demonstrating five use-case scenarios, in both individual and collaborative XR
applications across virtualities. Our technical evaluation and user studies
show that Explainable XR provides a highly usable analytics solution for
understanding user actions and delivering multifaceted, actionable insights
into user behaviors in immersive environments.
</summary>
    <author>
      <name>Yoonsang Kim</name>
    </author>
    <author>
      <name>Zainab Aamir</name>
    </author>
    <author>
      <name>Mithilesh Singh</name>
    </author>
    <author>
      <name>Saeed Boorboor</name>
    </author>
    <author>
      <name>Klaus Mueller</name>
    </author>
    <author>
      <name>Arie E. Kaufman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 8 figures. This is the author's version of the article that
  has been accepted for publication in IEEE Transactions on Visualization and
  Computer Graphics</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13773v1</id>
    <updated>2025-01-23T15:52:34Z</updated>
    <published>2025-01-23T15:52:34Z</published>
    <title>Do Large Language Models Truly Understand Geometric Structures?</title>
    <summary>  Geometric ability is a significant challenge for large language models (LLMs)
due to the need for advanced spatial comprehension and abstract thinking.
Existing datasets primarily evaluate LLMs on their final answers, but they
cannot truly measure their true understanding of geometric structures, as LLMs
can arrive at correct answers by coincidence. To fill this gap, we introduce
the GeomRel dataset, designed to evaluate LLMs' understanding of geometric
structures by isolating the core step of geometric relationship identification
in problem-solving. Using this benchmark, we conduct thorough evaluations of
diverse LLMs and identify key limitations in understanding geometric
structures. We further propose the Geometry Chain-of-Thought (GeoCoT) method,
which enhances LLMs' ability to identify geometric relationships, resulting in
significant performance improvements.
</summary>
    <author>
      <name>Xiaofeng Wang</name>
    </author>
    <author>
      <name>Yiming Wang</name>
    </author>
    <author>
      <name>Wenhong Zhu</name>
    </author>
    <author>
      <name>Rui Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13772v1</id>
    <updated>2025-01-23T15:51:38Z</updated>
    <published>2025-01-23T15:51:38Z</published>
    <title>Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits
  on Large Audio Language Models in Jailbreak</title>
    <summary>  Large Language Models (LLMs) demonstrate remarkable zero-shot performance
across various natural language processing tasks. The integration of multimodal
encoders extends their capabilities, enabling the development of Multimodal
Large Language Models that process vision, audio, and text. However, these
capabilities also raise significant security concerns, as these models can be
manipulated to generate harmful or inappropriate content through jailbreak.
While extensive research explores the impact of modality-specific input edits
on text-based LLMs and Large Vision-Language Models in jailbreak, the effects
of audio-specific edits on Large Audio-Language Models (LALMs) remain
underexplored. Hence, this paper addresses this gap by investigating how
audio-specific edits influence LALMs inference regarding jailbreak. We
introduce the Audio Editing Toolbox (AET), which enables audio-modality edits
such as tone adjustment, word emphasis, and noise injection, and the Edited
Audio Datasets (EADs), a comprehensive audio jailbreak benchmark. We also
conduct extensive evaluations of state-of-the-art LALMs to assess their
robustness under different audio edits. This work lays the groundwork for
future explorations on audio-modality interactions in LALMs security.
</summary>
    <author>
      <name>Erjia Xiao</name>
    </author>
    <author>
      <name>Hao Cheng</name>
    </author>
    <author>
      <name>Jing Shao</name>
    </author>
    <author>
      <name>Jinhao Duan</name>
    </author>
    <author>
      <name>Kaidi Xu</name>
    </author>
    <author>
      <name>Le Yang</name>
    </author>
    <author>
      <name>Jindong Gu</name>
    </author>
    <author>
      <name>Renjing Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13766v1</id>
    <updated>2025-01-23T15:46:43Z</updated>
    <published>2025-01-23T15:46:43Z</published>
    <title>UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level
  Mathematical Reasoning with Large Language Models</title>
    <summary>  Large Language Models (LLMs) have made significant strides in mathematical
reasoning, underscoring the need for a comprehensive and fair evaluation of
their capabilities. However, existing benchmarks often fall short, either
lacking extensive coverage of undergraduate-level mathematical problems or
probably suffering from test-set contamination. To address these issues, we
introduce UGMathBench, a diverse and dynamic benchmark specifically designed
for evaluating undergraduate-level mathematical reasoning with LLMs.
UGMathBench comprises 5,062 problems across 16 subjects and 111 topics,
featuring 10 distinct answer types. Each problem includes three randomized
versions, with additional versions planned for release as leading open-source
LLMs become saturated in UGMathBench. Furthermore, we propose two key metrics:
effective accuracy (EAcc), which measures the percentage of correctly solved
problems across all three versions, and reasoning gap ($\Delta$), which
assesses reasoning robustness by calculating the difference between the average
accuracy across all versions and EAcc. Our extensive evaluation of 23 leading
LLMs reveals that the highest EAcc achieved is 56.3\% by OpenAI-o1-mini, with
large $\Delta$ values observed across different models. This highlights the
need for future research aimed at developing "large reasoning models" with high
EAcc and $\Delta = 0$. We anticipate that the release of UGMathBench, along
with its detailed evaluation codes, will serve as a valuable resource to
advance the development of LLMs in solving mathematical problems.
</summary>
    <author>
      <name>Xin Xu</name>
    </author>
    <author>
      <name>Jiaxin Zhang</name>
    </author>
    <author>
      <name>Tianhao Chen</name>
    </author>
    <author>
      <name>Zitong Chao</name>
    </author>
    <author>
      <name>Jishan Hu</name>
    </author>
    <author>
      <name>Can Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICLR 2025</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Learning Representations (ICLR 2025)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2501.13766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13746v1</id>
    <updated>2025-01-23T15:22:25Z</updated>
    <published>2025-01-23T15:22:25Z</published>
    <title>EICopilot: Search and Explore Enterprise Information over Large-scale
  Knowledge Graphs with LLM-driven Agents</title>
    <summary>  The paper introduces EICopilot, an novel agent-based solution enhancing
search and exploration of enterprise registration data within extensive online
knowledge graphs like those detailing legal entities, registered capital, and
major shareholders. Traditional methods necessitate text-based queries and
manual subgraph explorations, often resulting in time-consuming processes.
EICopilot, deployed as a chatbot via Baidu Enterprise Search, improves this
landscape by utilizing Large Language Models (LLMs) to interpret natural
language queries. This solution automatically generates and executes Gremlin
scripts, providing efficient summaries of complex enterprise relationships.
Distinct feature a data pre-processing pipeline that compiles and annotates
representative queries into a vector database of examples for In-context
learning (ICL), a comprehensive reasoning pipeline combining Chain-of-Thought
with ICL to enhance Gremlin script generation for knowledge graph search and
exploration, and a novel query masking strategy that improves intent
recognition for heightened script accuracy. Empirical evaluations demonstrate
the superior performance of EICopilot, including speed and accuracy, over
baseline methods, with the \emph{Full Mask} variant achieving a syntax error
rate reduction to as low as 10.00% and an execution correctness of up to
82.14%. These components collectively contribute to superior querying
capabilities and summarization of intricate datasets, positioning EICopilot as
a groundbreaking tool in the exploration and exploitation of large-scale
knowledge graphs for enterprise information search.
</summary>
    <author>
      <name>Yuhui Yun</name>
    </author>
    <author>
      <name>Huilong Ye</name>
    </author>
    <author>
      <name>Xinru Li</name>
    </author>
    <author>
      <name>Ruojia Li</name>
    </author>
    <author>
      <name>Jingfeng Deng</name>
    </author>
    <author>
      <name>Li Li</name>
    </author>
    <author>
      <name>Haoyi Xiong</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13743v1</id>
    <updated>2025-01-23T15:18:22Z</updated>
    <published>2025-01-23T15:18:22Z</published>
    <title>GPT-HTree: A Decision Tree Framework Integrating Hierarchical Clustering
  and Large Language Models for Explainable Classification</title>
    <summary>  This paper introduces GPT-HTree, a framework combining hierarchical
clustering, decision trees, and large language models (LLMs) to address this
challenge. By leveraging hierarchical clustering to segment individuals based
on salient features, resampling techniques to balance class distributions, and
decision trees to tailor classification paths within each cluster, GPT-HTree
ensures both accuracy and interpretability. LLMs enhance the framework by
generating human-readable cluster descriptions, bridging quantitative analysis
with actionable insights.
</summary>
    <author>
      <name>Te Pei</name>
    </author>
    <author>
      <name>Fuat Alican</name>
    </author>
    <author>
      <name>Aaron Ontoyin Yin</name>
    </author>
    <author>
      <name>Yigit Ihlamur</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13742v1</id>
    <updated>2025-01-23T15:17:51Z</updated>
    <published>2025-01-23T15:17:51Z</published>
    <title>An Empirical Study of Retrieval-Augmented Code Generation: Challenges
  and Opportunities</title>
    <summary>  Code generation aims to automatically generate code snippets of specific
programming language according to natural language descriptions. The continuous
advancements in deep learning, particularly pre-trained models, have empowered
the code generation task to achieve remarkable performance. One main challenge
of pre-trained models for code generation is the semantic gap between natural
language requirements and source code. To address the issue, prior studies
typically adopt a retrieval-augmented framework for the task, where the similar
code snippets collected by a retrieval process can be leveraged to help
understand the requirements and provide guidance for the generation process.
However, there is a lack of systematic study on the application of this
framework for code generation, including the impact of the final generated
results and the specific usage of the framework. In this paper, we choose three
popular pre-trained code models, namely CodeGen, UniXcoder, and CodeT5, to
assess the impact of the quality and utilization of retrieved code on the
retrieval-augmented framework. Our analysis shows that the retrieval-augmented
framework is beneficial for improving the performance of the existing
pre-trained models. We also provide suggestions on the utilization of the
retrieval-augmented code generation framework: BM25 and Sequential Integration
Fusion are recommended due to their convenience and superior performance.
Sketch Filling Fusion, which extracts a sketch of relevant code, could help the
model improve its performance further. Additionally, we conduct experiments to
investigate the influence of the retrieval-augmented framework on large
language models for code generation, showing the effectiveness of the
framework, and we discuss the trade-off between performance improvement and
computational costs in each phase within the framework.
</summary>
    <author>
      <name>Zezhou Yang</name>
    </author>
    <author>
      <name>Sirong Chen</name>
    </author>
    <author>
      <name>Cuiyun Gao</name>
    </author>
    <author>
      <name>Zhenhao Li</name>
    </author>
    <author>
      <name>Xing Hu</name>
    </author>
    <author>
      <name>Kui Liu</name>
    </author>
    <author>
      <name>Xin Xia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accepted by TOSEM</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13731v1</id>
    <updated>2025-01-23T15:04:22Z</updated>
    <published>2025-01-23T15:04:22Z</published>
    <title>Pseudocode-Injection Magic: Enabling LLMs to Tackle Graph Computational
  Tasks</title>
    <summary>  Graph computational tasks are inherently challenging and often demand the
development of advanced algorithms for effective solutions. With the emergence
of large language models (LLMs), researchers have begun investigating their
potential to address these tasks. However, existing approaches are constrained
by LLMs' limited capability to comprehend complex graph structures and their
high inference costs, rendering them impractical for handling large-scale
graphs. Inspired by human approaches to graph problems, we introduce a novel
framework, PIE (Pseudocode-Injection-Enhanced LLM Reasoning for Graph
Computational Tasks), which consists of three key steps: problem understanding,
prompt design, and code generation. In this framework, LLMs are tasked with
understanding the problem and extracting relevant information to generate
correct code. The responsibility for analyzing the graph structure and
executing the code is delegated to the interpreter. We inject task-related
pseudocodes into the prompts to further assist the LLMs in generating efficient
code. We also employ cost-effective trial-and-error techniques to ensure that
the LLM-generated code executes correctly. Unlike other methods that require
invoking LLMs for each individual test case, PIE only calls the LLM during the
code generation phase, allowing the generated code to be reused and
significantly reducing inference costs. Extensive experiments demonstrate that
PIE outperforms existing baselines in terms of both accuracy and computational
efficiency.
</summary>
    <author>
      <name>Chang Gong</name>
    </author>
    <author>
      <name>Wanrui Bian</name>
    </author>
    <author>
      <name>Zhijie Zhang</name>
    </author>
    <author>
      <name>Weiguo Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13731v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13731v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13726v1</id>
    <updated>2025-01-23T14:58:56Z</updated>
    <published>2025-01-23T14:58:56Z</published>
    <title>RPO: Retrieval Preference Optimization for Robust Retrieval-Augmented
  Generation</title>
    <summary>  While Retrieval-Augmented Generation (RAG) has exhibited promise in utilizing
external knowledge, its generation process heavily depends on the quality and
accuracy of the retrieved context. Large language models (LLMs) struggle to
evaluate the correctness of non-parametric knowledge retrieved externally when
it differs from internal memorization, leading to knowledge conflicts during
response generation. To this end, we introduce the Retrieval Preference
Optimization (RPO), a lightweight and effective alignment method to adaptively
leverage multi-source knowledge based on retrieval relevance. An implicit
representation of retrieval relevance is derived and incorporated into the
reward model to integrate retrieval evaluation and response generation into a
single model, solving the problem that previous methods necessitate the
additional procedure to assess the retrieval quality. Notably, RPO is the only
RAG-dedicated alignment approach that quantifies the awareness of retrieval
relevance in training, overcoming mathematical obstacles. Experiments on four
datasets demonstrate that RPO outperforms RAG by 4-10% in accuracy without any
extra component, exhibiting its robust generalization.
</summary>
    <author>
      <name>Shi-Qi Yan</name>
    </author>
    <author>
      <name>Zhen-Hua Ling</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13720v1</id>
    <updated>2025-01-23T14:50:37Z</updated>
    <published>2025-01-23T14:50:37Z</published>
    <title>Musical ethnocentrism in Large Language Models</title>
    <summary>  Large Language Models (LLMs) reflect the biases in their training data and,
by extension, those of the people who created this training data. Detecting,
analyzing, and mitigating such biases is becoming a focus of research. One type
of bias that has been understudied so far are geocultural biases. Those can be
caused by an imbalance in the representation of different geographic regions
and cultures in the training data, but also by value judgments contained
therein. In this paper, we make a first step towards analyzing musical biases
in LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the
first, we prompt LLMs to provide lists of the "Top 100" musical contributors of
various categories and analyze their countries of origin. In the second
experiment, we ask the LLMs to numerically rate various aspects of the musical
cultures of different countries. Our results indicate a strong preference of
the LLMs for Western music cultures in both experiments.
</summary>
    <author>
      <name>Anna Kruspe</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 3rd Workshop on NLP for Music and Audio
  (NLP4MusA) 2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2501.13720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13707v1</id>
    <updated>2025-01-23T14:37:21Z</updated>
    <published>2025-01-23T14:37:21Z</published>
    <title>EventVL: Understand Event Streams via Multimodal Large Language Model</title>
    <summary>  The event-based Vision-Language Model (VLM) recently has made good progress
for practical vision tasks. However, most of these works just utilize CLIP for
focusing on traditional perception tasks, which obstruct model understanding
explicitly the sufficient semantics and context from event streams. To address
the deficiency, we propose EventVL, the first generative event-based MLLM
(Multimodal Large Language Model) framework for explicit semantic
understanding. Specifically, to bridge the data gap for connecting different
modalities semantics, we first annotate a large event-image/video-text dataset,
containing almost 1.4 million high-quality pairs of data, which enables
effective learning across various scenes, e.g., drive scene or human motion.
After that, we design Event Spatiotemporal Representation to fully explore the
comprehensive information by diversely aggregating and segmenting the event
stream. To further promote a compact semantic space, Dynamic Semantic Alignment
is introduced to improve and complete sparse semantic spaces of events.
Extensive experiments show that our EventVL can significantly surpass existing
MLLM baselines in event captioning and scene description generation tasks. We
hope our research could contribute to the development of the event vision
community.
</summary>
    <author>
      <name>Pengteng Li</name>
    </author>
    <author>
      <name>Yunfan Lu</name>
    </author>
    <author>
      <name>Pinghao Song</name>
    </author>
    <author>
      <name>Wuyang Li</name>
    </author>
    <author>
      <name>Huizai Yao</name>
    </author>
    <author>
      <name>Hui Xiong</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13699v1</id>
    <updated>2025-01-23T14:27:11Z</updated>
    <published>2025-01-23T14:27:11Z</published>
    <title>DI-BENCH: Benchmarking Large Language Models on Dependency Inference
  with Testable Repositories at Scale</title>
    <summary>  Large Language Models have advanced automated software development, however,
it remains a challenge to correctly infer dependencies, namely, identifying the
internal components and external packages required for a repository to
successfully run. Existing studies highlight that dependency-related issues
cause over 40\% of observed runtime errors on the generated repository. To
address this, we introduce DI-BENCH, a large-scale benchmark and evaluation
framework specifically designed to assess LLMs' capability on dependency
inference. The benchmark features 581 repositories with testing environments
across Python, C#, Rust, and JavaScript. Extensive experiments with textual and
execution-based metrics reveal that the current best-performing model achieves
only a 42.9% execution pass rate, indicating significant room for improvement.
DI-BENCH establishes a new viewpoint for evaluating LLM performance on
repositories, paving the way for more robust end-to-end software synthesis.
</summary>
    <author>
      <name>Linghao Zhang</name>
    </author>
    <author>
      <name>Junhao Wang</name>
    </author>
    <author>
      <name>Shilin He</name>
    </author>
    <author>
      <name>Chaoyun Zhang</name>
    </author>
    <author>
      <name>Yu Kang</name>
    </author>
    <author>
      <name>Bowen Li</name>
    </author>
    <author>
      <name>Jiaheng Wen</name>
    </author>
    <author>
      <name>Chengxing Xie</name>
    </author>
    <author>
      <name>Maoquan Wang</name>
    </author>
    <author>
      <name>Yufan Huang</name>
    </author>
    <author>
      <name>Elsie Nallipogu</name>
    </author>
    <author>
      <name>Qingwei Lin</name>
    </author>
    <author>
      <name>Yingnong Dang</name>
    </author>
    <author>
      <name>Saravan Rajmohan</name>
    </author>
    <author>
      <name>Dongmei Zhang</name>
    </author>
    <author>
      <name>Qi Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13687v1</id>
    <updated>2025-01-23T14:13:56Z</updated>
    <published>2025-01-23T14:13:56Z</published>
    <title>Question Answering on Patient Medical Records with Private Fine-Tuned
  LLMs</title>
    <summary>  Healthcare systems continuously generate vast amounts of electronic health
records (EHRs), commonly stored in the Fast Healthcare Interoperability
Resources (FHIR) standard. Despite the wealth of information in these records,
their complexity and volume make it difficult for users to retrieve and
interpret crucial health insights. Recent advances in Large Language Models
(LLMs) offer a solution, enabling semantic question answering (QA) over medical
data, allowing users to interact with their health records more effectively.
However, ensuring privacy and compliance requires edge and private deployments
of LLMs.
  This paper proposes a novel approach to semantic QA over EHRs by first
identifying the most relevant FHIR resources for a user query (Task1) and
subsequently answering the query based on these resources (Task2). We explore
the performance of privately hosted, fine-tuned LLMs, evaluating them against
benchmark models such as GPT-4 and GPT-4o. Our results demonstrate that
fine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by
0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we
examine advanced aspects of LLM usage, including sequential fine-tuning, model
self-evaluation (narcissistic evaluation), and the impact of training data size
on performance. The models and datasets are available here:
https://huggingface.co/genloop
</summary>
    <author>
      <name>Sara Kothari</name>
    </author>
    <author>
      <name>Ayush Gupta</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13677v1</id>
    <updated>2025-01-23T14:02:51Z</updated>
    <published>2025-01-23T14:02:51Z</published>
    <title>HumorReject: Decoupling LLM Safety from Refusal Prefix via A Little
  Humor</title>
    <summary>  Large Language Models (LLMs) commonly rely on explicit refusal prefixes for
safety, making them vulnerable to prefix injection attacks. We introduce
HumorReject, a novel data-driven approach that fundamentally reimagines LLM
safety by decoupling it from refusal prefixes through the use of humor as an
indirect refusal strategy. Rather than explicitly rejecting harmful
instructions, HumorReject responds with contextually appropriate humor that
naturally defuses potentially dangerous requests while maintaining engaging
interactions. Our approach effectively addresses the common "over-defense"
issues in existing safety mechanisms, demonstrating superior robustness against
various attack vectors while preserving natural and high-quality interactions
on legitimate tasks. Our findings suggest that innovations at the data level
are even more fundamental than the alignment algorithm itself in achieving
effective LLM safety, opening new directions for developing more resilient and
user-friendly AI systems.
</summary>
    <author>
      <name>Zihui Wu</name>
    </author>
    <author>
      <name>Haichang Gao</name>
    </author>
    <author>
      <name>Jiacheng Luo</name>
    </author>
    <author>
      <name>Zhaoxiang Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13669v1</id>
    <updated>2025-01-23T13:54:53Z</updated>
    <published>2025-01-23T13:54:53Z</published>
    <title>How to Complete Domain Tuning while Keeping General Ability in LLM:
  Adaptive Layer-wise and Element-wise Regularization</title>
    <summary>  Large Language Models (LLMs) exhibit strong general-purpose language
capabilities. However, fine-tuning these models on domain-specific tasks often
leads to catastrophic forgetting, where the model overwrites or loses essential
knowledge acquired during pretraining. This phenomenon significantly limits the
broader applicability of LLMs. To address this challenge, we propose a novel
approach to compute the element-wise importance of model parameters crucial for
preserving general knowledge during fine-tuning. Our method utilizes a
dual-objective optimization strategy: (1) regularization loss to retain the
parameter crucial for general knowledge; (2) cross-entropy loss to adapt to
domain-specific tasks. Additionally, we introduce layer-wise coefficients to
account for the varying contributions of different layers, dynamically
balancing the dual-objective optimization. Extensive experiments on scientific,
medical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our
approach mitigates catastrophic forgetting while enhancing model adaptability.
Compared to previous methods, our solution is approximately 20 times faster and
requires only 10%-15% of the storage, highlighting the practical efficiency.
The code will be released.
</summary>
    <author>
      <name>Shezheng Song</name>
    </author>
    <author>
      <name>Hao Xu</name>
    </author>
    <author>
      <name>Jun Ma</name>
    </author>
    <author>
      <name>Shasha Li</name>
    </author>
    <author>
      <name>Long Peng</name>
    </author>
    <author>
      <name>Qian Wan</name>
    </author>
    <author>
      <name>Xiaodong Liu</name>
    </author>
    <author>
      <name>Jie Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13652v1</id>
    <updated>2025-01-23T13:31:51Z</updated>
    <published>2025-01-23T13:31:51Z</published>
    <title>LVPruning: An Effective yet Simple Language-Guided Vision Token Pruning
  Approach for Multi-modal Large Language Models</title>
    <summary>  Multi-modal Large Language Models (MLLMs) have achieved remarkable success by
integrating visual and textual modalities. However, they incur significant
computational overhead due to the large number of vision tokens processed,
limiting their practicality in resource-constrained environments. We introduce
Language-Guided Vision Token Pruning (LVPruning) for MLLMs, an effective yet
simple method that significantly reduces the computational burden while
preserving model performance. LVPruning employs cross-attention modules to
compute the importance of vision tokens based on their interaction with
language tokens, determining which to prune. Importantly, LVPruning can be
integrated without modifying the original MLLM parameters, which makes
LVPruning simple to apply or remove. Our experiments show that LVPruning can
effectively reduce up to 90% of vision tokens by the middle layer of LLaVA-1.5,
resulting in a 62.1% decrease in inference Tera Floating-Point Operations Per
Second (TFLOPs), with an average performance loss of just 0.45% across nine
multi-modal benchmarks.
</summary>
    <author>
      <name>Yizheng Sun</name>
    </author>
    <author>
      <name>Yanze Xin</name>
    </author>
    <author>
      <name>Hao Li</name>
    </author>
    <author>
      <name>Jingyuan Sun</name>
    </author>
    <author>
      <name>Chenghua Lin</name>
    </author>
    <author>
      <name>Riza Batista-Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13629v1</id>
    <updated>2025-01-23T12:58:14Z</updated>
    <published>2025-01-23T12:58:14Z</published>
    <title>Sigma: Differential Rescaling of Query, Key and Value for Efficient
  Language Models</title>
    <summary>  We introduce Sigma, an efficient large language model specialized for the
system domain, empowered by a novel architecture including DiffQKV attention,
and pre-trained on our meticulously collected system domain data. DiffQKV
attention significantly enhances the inference efficiency of Sigma by
optimizing the Query (Q), Key (K), and Value (V) components in the attention
mechanism differentially, based on their varying impacts on the model
performance and efficiency indicators. Specifically, we (1) conduct extensive
experiments that demonstrate the model's varying sensitivity to the compression
of K and V components, leading to the development of differentially compressed
KV, and (2) propose augmented Q to expand the Q head dimension, which enhances
the model's representation capacity with minimal impacts on the inference
speed. Rigorous theoretical and empirical analyses reveal that DiffQKV
attention significantly enhances efficiency, achieving up to a 33.36%
improvement in inference speed over the conventional grouped-query attention
(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various
sources, including 19.5B system domain data that we carefully collect and 1T
tokens of synthesized and rewritten data. In general domains, Sigma achieves
comparable performance to other state-of-arts models. In the system domain, we
introduce the first comprehensive benchmark AIMicius, where Sigma demonstrates
remarkable performance across all tasks, significantly outperforming GPT-4 with
an absolute improvement up to 52.5%.
</summary>
    <author>
      <name>Zhenghao Lin</name>
    </author>
    <author>
      <name>Zihao Tang</name>
    </author>
    <author>
      <name>Xiao Liu</name>
    </author>
    <author>
      <name>Yeyun Gong</name>
    </author>
    <author>
      <name>Yi Cheng</name>
    </author>
    <author>
      <name>Qi Chen</name>
    </author>
    <author>
      <name>Hang Li</name>
    </author>
    <author>
      <name>Ying Xin</name>
    </author>
    <author>
      <name>Ziyue Yang</name>
    </author>
    <author>
      <name>Kailai Yang</name>
    </author>
    <author>
      <name>Yu Yan</name>
    </author>
    <author>
      <name>Xiao Liang</name>
    </author>
    <author>
      <name>Shuai Lu</name>
    </author>
    <author>
      <name>Yiming Huang</name>
    </author>
    <author>
      <name>Zheheng Luo</name>
    </author>
    <author>
      <name>Lei Qu</name>
    </author>
    <author>
      <name>Xuan Feng</name>
    </author>
    <author>
      <name>Yaoxiang Wang</name>
    </author>
    <author>
      <name>Yuqing Xia</name>
    </author>
    <author>
      <name>Feiyang Chen</name>
    </author>
    <author>
      <name>Yuting Jiang</name>
    </author>
    <author>
      <name>Yasen Hu</name>
    </author>
    <author>
      <name>Hao Ni</name>
    </author>
    <author>
      <name>Binyang Li</name>
    </author>
    <author>
      <name>Guoshuai Zhao</name>
    </author>
    <author>
      <name>Jui-Hao Chiang</name>
    </author>
    <author>
      <name>Zhongxin Guo</name>
    </author>
    <author>
      <name>Chen Lin</name>
    </author>
    <author>
      <name>Kun Kuang</name>
    </author>
    <author>
      <name>Wenjie Li</name>
    </author>
    <author>
      <name>Yelong Shen</name>
    </author>
    <author>
      <name>Jian Jiao</name>
    </author>
    <author>
      <name>Peng Cheng</name>
    </author>
    <author>
      <name>Mao Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13629v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13629v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13604v1</id>
    <updated>2025-01-23T12:12:59Z</updated>
    <published>2025-01-23T12:12:59Z</published>
    <title>FedPref: Federated Learning Across Heterogeneous Multi-objective
  Preferences</title>
    <summary>  Federated Learning (FL) is a distributed machine learning strategy, developed
for settings where training data is owned by distributed devices and cannot be
shared. FL circumvents this constraint by carrying out model training in
distribution. The parameters of these local models are shared intermittently
among participants and aggregated to enhance model accuracy. This strategy has
been rapidly adopted by the industry in efforts to overcome privacy and
resource constraints in model training. However, the application of FL to
real-world settings brings additional challenges associated with heterogeneity
between participants. Research into mitigating these difficulties in FL has
largely focused on only two types of heterogeneity: the unbalanced distribution
of training data, and differences in client resources. Yet more types of
heterogeneity are becoming relevant as the capability of FL expands to cover
more complex problems, from the tuning of LLMs to enabling machine learning on
edge devices. In this work, we discuss a novel type of heterogeneity that is
likely to become increasingly relevant in future applications: this is
preference heterogeneity, emerging when clients learn under multiple
objectives, with different importance assigned to each objective on different
clients. In this work, we discuss the implications of this type of
heterogeneity and propose FedPref, a first algorithm designed to facilitate
personalised FL in this setting. We demonstrate the effectiveness of the
algorithm across different problems, preference distributions and model
architectures. In addition, we introduce a new analytical point of view, based
on multi-objective metrics, for evaluating the performance of FL algorithms in
this setting beyond the traditional client-focused metrics. We perform a second
experimental analysis based in this view, and show that FedPref outperforms
compared algorithms.
</summary>
    <author>
      <name>Maria Hartmann</name>
    </author>
    <author>
      <name>Grégoire Danoy</name>
    </author>
    <author>
      <name>Pascal Bouvry</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3708984</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3708984" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACM ToMPECS journal</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13594v1</id>
    <updated>2025-01-23T12:03:29Z</updated>
    <published>2025-01-23T12:03:29Z</published>
    <title>Text-to-SQL based on Large Language Models and Database Keyword Search</title>
    <summary>  Text-to-SQL prompt strategies based on Large Language Models (LLMs) achieve
remarkable performance on well-known benchmarks. However, when applied to
real-world databases, their performance is significantly less than for these
benchmarks, especially for Natural Language (NL) questions requiring complex
filters and joins to be processed. This paper then proposes a strategy to
compile NL questions into SQL queries that incorporates a dynamic few-shot
examples strategy and leverages the services provided by a database keyword
search (KwS) platform. The paper details how the precision and recall of the
schema-linking process are improved with the help of the examples provided and
the keyword-matching service that the KwS platform offers. Then, it shows how
the KwS platform can be used to synthesize a view that captures the joins
required to process an input NL question and thereby simplify the SQL query
compilation step. The paper includes experiments with a real-world relational
database to assess the performance of the proposed strategy. The experiments
suggest that the strategy achieves an accuracy on the real-world relational
database that surpasses state-of-the-art approaches. The paper concludes by
discussing the results obtained.
</summary>
    <author>
      <name>Eduardo R. Nascimento</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Instituto Tecgraf, PUC-Rio, Rio de Janeiro, Brazil</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Departamento de Informática, PUC-Rio, Rio de Janeiro, Brazil</arxiv:affiliation>
    </author>
    <author>
      <name>Caio Viktor S. Avila</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Instituto Tecgraf, PUC-Rio, Rio de Janeiro, Brazil</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Departamento de Computação, UFC, Fortaleza, Brazil</arxiv:affiliation>
    </author>
    <author>
      <name>Yenier T. Izquierdo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Instituto Tecgraf, PUC-Rio, Rio de Janeiro, Brazil</arxiv:affiliation>
    </author>
    <author>
      <name>Grettel M. García</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Instituto Tecgraf, PUC-Rio, Rio de Janeiro, Brazil</arxiv:affiliation>
    </author>
    <author>
      <name>Lucas Feijó L. Andrade</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Instituto Tecgraf, PUC-Rio, Rio de Janeiro, Brazil</arxiv:affiliation>
    </author>
    <author>
      <name>Michelle S. P. Facina</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Petrobras, Rio de Janeiro, Brazil</arxiv:affiliation>
    </author>
    <author>
      <name>Melissa Lemos</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Instituto Tecgraf, PUC-Rio, Rio de Janeiro, Brazil</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Departamento de Informática, PUC-Rio, Rio de Janeiro, Brazil</arxiv:affiliation>
    </author>
    <author>
      <name>Marco A. Casanova</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Instituto Tecgraf, PUC-Rio, Rio de Janeiro, Brazil</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Departamento de Informática, PUC-Rio, Rio de Janeiro, Brazil</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2501.13594v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13594v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13573v1</id>
    <updated>2025-01-23T11:23:25Z</updated>
    <published>2025-01-23T11:23:25Z</published>
    <title>Improving Contextual Faithfulness of Large Language Models via Retrieval
  Heads-Induced Optimization</title>
    <summary>  Ensuring contextual faithfulness in retrieval-augmented large language models
(LLMs) is crucial for building trustworthy information-seeking systems,
particularly in long-form question-answering (LFQA) scenarios. In this work, we
identify a salient correlation between LFQA faithfulness and retrieval heads, a
set of attention heads responsible for retrieving contextual information.
Leveraging this insight, we propose RHIO, a framework designed to teach LLMs to
explicitly discriminate between faithful and unfaithful generations. RHIO first
augments unfaithful samples that simulate realistic model-intrinsic errors by
selectively masking retrieval heads. Then, these samples are incorporated into
joint training, enabling the model to distinguish unfaithful outputs from
faithful ones conditioned on control tokens. Furthermore, these control tokens
are leveraged to self-induce contrastive outputs, amplifying their difference
through contrastive decoding. Additionally, to facilitate the evaluation of
contextual faithfulness, we also introduce GroundBench, a comprehensive
benchmark compiled from five existing LFQA datasets. Extensive experimental
results on GroundBench demonstrate that RHIO significantly improves
faithfulness, even outperforming GPT-4o.
</summary>
    <author>
      <name>Lei Huang</name>
    </author>
    <author>
      <name>Xiaocheng Feng</name>
    </author>
    <author>
      <name>Weitao Ma</name>
    </author>
    <author>
      <name>Yuchun Fan</name>
    </author>
    <author>
      <name>Xiachong Feng</name>
    </author>
    <author>
      <name>Yangfan Ye</name>
    </author>
    <author>
      <name>Weihong Zhong</name>
    </author>
    <author>
      <name>Yuxuan Gu</name>
    </author>
    <author>
      <name>Baoxin Wang</name>
    </author>
    <author>
      <name>Dayong Wu</name>
    </author>
    <author>
      <name>Guoping Hu</name>
    </author>
    <author>
      <name>Bing Qin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ARR October 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13545v1</id>
    <updated>2025-01-23T10:46:14Z</updated>
    <published>2025-01-23T10:46:14Z</published>
    <title>LLMs Can Plan Only If We Tell Them</title>
    <summary>  Large language models (LLMs) have demonstrated significant capabilities in
natural language processing and reasoning, yet their effectiveness in
autonomous planning has been under debate. While existing studies have utilized
LLMs with external feedback mechanisms or in controlled environments for
planning, these approaches often involve substantial computational and
development resources due to the requirement for careful design and iterative
backprompting. Moreover, even the most advanced LLMs like GPT-4 struggle to
match human performance on standard planning benchmarks, such as the
Blocksworld, without additional support. This paper investigates whether LLMs
can independently generate long-horizon plans that rival human baselines. Our
novel enhancements to Algorithm-of-Thoughts (AoT), which we dub AoT+, help
achieve state-of-the-art results in planning benchmarks out-competing prior
methods and human baselines all autonomously.
</summary>
    <author>
      <name>Bilgehan Sel</name>
    </author>
    <author>
      <name>Ruoxi Jia</name>
    </author>
    <author>
      <name>Ming Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13545v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13545v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13536v1</id>
    <updated>2025-01-23T10:35:22Z</updated>
    <published>2025-01-23T10:35:22Z</published>
    <title>ReasVQA: Advancing VideoQA with Imperfect Reasoning Process</title>
    <summary>  Video Question Answering (VideoQA) is a challenging task that requires
understanding complex visual and temporal relationships within videos to answer
questions accurately. In this work, we introduce \textbf{ReasVQA}
(Reasoning-enhanced Video Question Answering), a novel approach that leverages
reasoning processes generated by Multimodal Large Language Models (MLLMs) to
improve the performance of VideoQA models. Our approach consists of three
phases: reasoning generation, reasoning refinement, and learning from
reasoning. First, we generate detailed reasoning processes using additional
MLLMs, and second refine them via a filtering step to ensure data quality.
Finally, we use the reasoning data, which might be in an imperfect form, to
guide the VideoQA model via multi-task learning, on how to interpret and answer
questions based on a given video. We evaluate ReasVQA on three popular
benchmarks, and our results establish new state-of-the-art performance with
significant improvements of +2.9 on NExT-QA, +7.3 on STAR, and +5.9 on
IntentQA. Our findings demonstrate the supervising benefits of integrating
reasoning processes into VideoQA. Further studies validate each component of
our method, also with different backbones and MLLMs, and again highlight the
advantages of this simple but effective method. We offer a new perspective on
enhancing VideoQA performance by utilizing advanced reasoning techniques,
setting a new benchmark in this research field.
</summary>
    <author>
      <name>Jianxin Liang</name>
    </author>
    <author>
      <name>Xiaojun Meng</name>
    </author>
    <author>
      <name>Huishuai Zhang</name>
    </author>
    <author>
      <name>Yueqian Wang</name>
    </author>
    <author>
      <name>Jiansheng Wei</name>
    </author>
    <author>
      <name>Dongyan Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to main conference at NAACL 2025; 8 pages;</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13536v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13536v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13491v1</id>
    <updated>2025-01-23T09:14:07Z</updated>
    <published>2025-01-23T09:14:07Z</published>
    <title>RECALL: Library-Like Behavior In Language Models is Enhanced by
  Self-Referencing Causal Cycles</title>
    <summary>  We introduce the concept of the self-referencing causal cycle (abbreviated
RECALL) - a mechanism that enables large language models (LLMs) to bypass the
limitations of unidirectional causality, which underlies a phenomenon known as
the reversal curse. When an LLM is prompted with sequential data, it often
fails to recall preceding context. For example, when we ask an LLM to recall
the line preceding "O say does that star-spangled banner yet wave" in the U.S.
National Anthem, it often fails to correctly return "Gave proof through the
night that our flag was still there" - this is due to the reversal curse. It
occurs because language models such as ChatGPT and Llama generate text based on
preceding tokens, requiring facts to be learned and reproduced in a consistent
token order. While the reversal curse is often viewed as a limitation, we offer
evidence of an alternative view: it is not always an obstacle in practice. We
find that RECALL is driven by what we designate as cycle tokens - sequences
that connect different parts of the training data, enabling recall of preceding
tokens from succeeding ones. Through rigorous probabilistic formalization and
controlled experiments, we demonstrate how the cycles they induce influence a
model's ability to reproduce information. To facilitate reproducibility, we
provide our code and experimental details at
https://anonymous.4open.science/r/remember-B0B8/.
</summary>
    <author>
      <name>Munachiso Nwadike</name>
    </author>
    <author>
      <name>Zangir Iklassov</name>
    </author>
    <author>
      <name>Toluwani Aremu</name>
    </author>
    <author>
      <name>Tatsuya Hiraoka</name>
    </author>
    <author>
      <name>Velibor Bojkovic</name>
    </author>
    <author>
      <name>Benjamin Heinzerling</name>
    </author>
    <author>
      <name>Hilal Alqaubeh</name>
    </author>
    <author>
      <name>Martin Takáč</name>
    </author>
    <author>
      <name>Kentaro Inui</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13491v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13491v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13480v1</id>
    <updated>2025-01-23T08:53:12Z</updated>
    <published>2025-01-23T08:53:12Z</published>
    <title>Adaptive Testing for LLM-Based Applications: A Diversity-based Approach</title>
    <summary>  The recent surge of building software systems powered by Large Language
Models (LLMs) has led to the development of various testing frameworks,
primarily focused on treating prompt templates as the unit of testing. Despite
the significant costs associated with test input execution and output
assessment, the curation of optimized test suites is yet overlooked in these
tools, which calls for tailored test selection or prioritization strategies. In
this paper, we show that diversity-based testing techniques, such as Adaptive
Random Testing (ART) with appropriate string distance metrics, can be
effectively applied to the testing of prompt templates. Our proposed adaptive
testing approach adjusts the conventional ART process to this context by
selecting new test inputs based on scores derived from existing test suite and
their labelling results. Our results, obtained using various implementations
that explore several string-based distances, confirm that our approach enables
the discovery of failures with reduced testing budgets and promotes the
generation of more varied outputs.
</summary>
    <author>
      <name>Juyeon Yoon</name>
    </author>
    <author>
      <name>Robert Feldt</name>
    </author>
    <author>
      <name>Shin Yoo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13468v1</id>
    <updated>2025-01-23T08:33:10Z</updated>
    <published>2025-01-23T08:33:10Z</published>
    <title>Streaming Video Understanding and Multi-round Interaction with
  Memory-enhanced Knowledge</title>
    <summary>  Recent advances in Large Language Models (LLMs) have enabled the development
of Video-LLMs, advancing multimodal learning by bridging video data with
language tasks. However, current video understanding models struggle with
processing long video sequences, supporting multi-turn dialogues, and adapting
to real-world dynamic scenarios. To address these issues, we propose
StreamChat, a training-free framework for streaming video reasoning and
conversational interaction. $\StreamChat$ leverages a novel hierarchical memory
system to efficiently process and compress video features over extended
sequences, enabling real-time, multi-turn dialogue. Our framework incorporates
a parallel system scheduling strategy that enhances processing speed and
reduces latency, ensuring robust performance in real-world applications.
Furthermore, we introduce StreamBench, a versatile benchmark that evaluates
streaming video understanding across diverse media types and interactive
scenarios, including multi-turn interactions and complex reasoning tasks.
Extensive evaluations on StreamBench and other public benchmarks demonstrate
that StreamChat significantly outperforms existing state-of-the-art models in
terms of accuracy and response times, confirming its effectiveness for
streaming video understanding. Code is available at StreamChat:
https://github.com/hmxiong/StreamChat.
</summary>
    <author>
      <name>Haomiao Xiong</name>
    </author>
    <author>
      <name>Zongxin Yang</name>
    </author>
    <author>
      <name>Jiazuo Yu</name>
    </author>
    <author>
      <name>Yunzhi Zhuge</name>
    </author>
    <author>
      <name>Lu Zhang</name>
    </author>
    <author>
      <name>Jiawen Zhu</name>
    </author>
    <author>
      <name>Huchuan Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICLR 2025. Code is available at
  https://github.com/hmxiong/StreamChat</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13453v1</id>
    <updated>2025-01-23T08:09:54Z</updated>
    <published>2025-01-23T08:09:54Z</published>
    <title>Spurious Forgetting in Continual Learning of Language Models</title>
    <summary>  Recent advancements in large language models (LLMs) reveal a perplexing
phenomenon in continual learning: despite extensive training, models experience
significant performance declines, raising questions about task alignment and
underlying knowledge retention. This study first explores the concept of
"spurious forgetting", proposing that such performance drops often reflect a
decline in task alignment rather than true knowledge loss. Through controlled
experiments with a synthesized dataset, we investigate the dynamics of model
performance during the initial training phases of new tasks, discovering that
early optimization steps can disrupt previously established task alignments.
Our theoretical analysis connects these shifts to orthogonal updates in model
weights, providing a robust framework for understanding this behavior.
Ultimately, we introduce a Freezing strategy that fix the bottom layers of the
model, leading to substantial improvements in four continual learning
scenarios. Our findings underscore the critical distinction between task
alignment and knowledge retention, paving the way for more effective strategies
in continual learning.
</summary>
    <author>
      <name>Junhao Zheng</name>
    </author>
    <author>
      <name>Xidi Cai</name>
    </author>
    <author>
      <name>Shengjie Qiu</name>
    </author>
    <author>
      <name>Qianli Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13449v1</id>
    <updated>2025-01-23T08:02:59Z</updated>
    <published>2025-01-23T08:02:59Z</published>
    <title>MultiDreamer3D: Multi-concept 3D Customization with Concept-Aware
  Diffusion Guidance</title>
    <summary>  While single-concept customization has been studied in 3D, multi-concept
customization remains largely unexplored. To address this, we propose
MultiDreamer3D that can generate coherent multi-concept 3D content in a
divide-and-conquer manner. First, we generate 3D bounding boxes using an
LLM-based layout controller. Next, a selective point cloud generator creates
coarse point clouds for each concept. These point clouds are placed in the 3D
bounding boxes and initialized into 3D Gaussian Splatting with concept labels,
enabling precise identification of concept attributions in 2D projections.
Finally, we refine 3D Gaussians via concept-aware interval score matching,
guided by concept-aware diffusion. Our experimental results show that
MultiDreamer3D not only ensures object presence and preserves the distinct
identities of each concept but also successfully handles complex cases such as
property change or interaction. To the best of our knowledge, we are the first
to address the multi-concept customization in 3D.
</summary>
    <author>
      <name>Wooseok Song</name>
    </author>
    <author>
      <name>Seunggyu Chang</name>
    </author>
    <author>
      <name>Jaejun Yoo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13428v1</id>
    <updated>2025-01-23T07:21:08Z</updated>
    <published>2025-01-23T07:21:08Z</published>
    <title>Softplus Attention with Re-weighting Boosts Length Extrapolation in
  Large Language Models</title>
    <summary>  Large language models have achieved remarkable success in recent years,
primarily due to the implementation of self-attention mechanisms. However,
traditional Softmax attention suffers from numerical instability and reduced
performance as the length of inference tokens increases. This paper addresses
these issues by decomposing the Softmax operation into a non-linear
transformation and the $l_1$-norm. We identify the latter as essential for
maintaining model performance. By replacing the non-linear transformation with
the Softplus activation function and introducing a dynamic length scale factor
for different token lengths based on invariance entropy, we create a novel
attention mechanism with performance better than conventional Softmax attention
across various inference lengths. To further improve the length extrapolation
ability of the proposed attention mechanism, we introduce a re-weighting
mechanism that amplifies significant attention weights while diminishing weaker
ones, enabling the model to concentrate more effectively on relevant tokens.
When combined with our proposed attention mechanism, this approach demonstrates
significant promise in managing longer sequences, maintaining nearly constant
validation loss even at 16$\times$ the training token length while ensuring
numerical stability. Our code is available at:
https://github.com/iminfine/freeatten.
</summary>
    <author>
      <name>Bo Gao</name>
    </author>
    <author>
      <name>Michael W. Spratling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages and 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13411v1</id>
    <updated>2025-01-23T06:33:05Z</updated>
    <published>2025-01-23T06:33:05Z</published>
    <title>VulnBot: Autonomous Penetration Testing for A Multi-Agent Collaborative
  Framework</title>
    <summary>  Penetration testing is a vital practice for identifying and mitigating
vulnerabilities in cybersecurity systems, but its manual execution is
labor-intensive and time-consuming. Existing large language model
(LLM)-assisted or automated penetration testing approaches often suffer from
inefficiencies, such as a lack of contextual understanding and excessive,
unstructured data generation. This paper presents VulnBot, an automated
penetration testing framework that leverages LLMs to simulate the collaborative
workflow of human penetration testing teams through a multi-agent system. To
address the inefficiencies and reliance on manual intervention in traditional
penetration testing methods, VulnBot decomposes complex tasks into three
specialized phases: reconnaissance, scanning, and exploitation. These phases
are guided by a penetration task graph (PTG) to ensure logical task execution.
Key design features include role specialization, penetration path planning,
inter-agent communication, and generative penetration behavior. Experimental
results demonstrate that VulnBot outperforms baseline models such as GPT-4 and
Llama3 in automated penetration testing tasks, particularly showcasing its
potential in fully autonomous testing on real-world machines.
</summary>
    <author>
      <name>He Kong</name>
    </author>
    <author>
      <name>Die Hu</name>
    </author>
    <author>
      <name>Jingguo Ge</name>
    </author>
    <author>
      <name>Liangxiong Li</name>
    </author>
    <author>
      <name>Tong Li</name>
    </author>
    <author>
      <name>Bingzhen Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13391v1</id>
    <updated>2025-01-23T05:24:18Z</updated>
    <published>2025-01-23T05:24:18Z</published>
    <title>Can Large Language Models Understand Preferences in Personalized
  Recommendation?</title>
    <summary>  Large Language Models (LLMs) excel in various tasks, including personalized
recommendations. Existing evaluation methods often focus on rating prediction,
relying on regression errors between actual and predicted ratings. However,
user rating bias and item quality, two influential factors behind rating
scores, can obscure personal preferences in user-item pair data. To address
this, we introduce PerRecBench, disassociating the evaluation from these two
factors and assessing recommendation techniques on capturing the personal
preferences in a grouped ranking manner. We find that the LLM-based
recommendation techniques that are generally good at rating prediction fail to
identify users' favored and disfavored items when the user rating bias and item
quality are eliminated by grouping users. With PerRecBench and 19 LLMs, we find
that while larger models generally outperform smaller ones, they still struggle
with personalized recommendation. Our findings reveal the superiority of
pairwise and listwise ranking approaches over pointwise ranking, PerRecBench's
low correlation with traditional regression metrics, the importance of user
profiles, and the role of pretraining data distributions. We further explore
three supervised fine-tuning strategies, finding that merging weights from
single-format training is promising but improving LLMs' understanding of user
preferences remains an open research problem. Code and data are available at
https://github.com/TamSiuhin/PerRecBench
</summary>
    <author>
      <name>Zhaoxuan Tan</name>
    </author>
    <author>
      <name>Zinan Zeng</name>
    </author>
    <author>
      <name>Qingkai Zeng</name>
    </author>
    <author>
      <name>Zhenyu Wu</name>
    </author>
    <author>
      <name>Zheyuan Liu</name>
    </author>
    <author>
      <name>Fengran Mo</name>
    </author>
    <author>
      <name>Meng Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13391v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13391v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13381v1</id>
    <updated>2025-01-23T04:50:03Z</updated>
    <published>2025-01-23T04:50:03Z</published>
    <title>Do as We Do, Not as You Think: the Conformity of Large Language Models</title>
    <summary>  Recent advancements in large language models (LLMs) revolutionize the field
of intelligent agents, enabling collaborative multi-agent systems capable of
tackling complex problems across various domains. However, the potential of
conformity within these systems, analogous to phenomena like conformity bias
and groupthink in human group dynamics, remains largely unexplored, raising
concerns about their collective problem-solving capabilities and possible
ethical implications. This paper presents a comprehensive study on conformity
in LLM-driven multi-agent systems, focusing on three aspects: the existence of
conformity, the factors influencing conformity, and potential mitigation
strategies. In particular, we introduce BenchForm, a new conformity-oriented
benchmark, featuring reasoning-intensive tasks and five distinct interaction
protocols designed to probe LLMs' behavior in collaborative scenarios. Several
representative LLMs are evaluated on BenchForm, using metrics such as
conformity rate and independence rate to quantify conformity's impact. Our
analysis delves into factors influencing conformity, including interaction time
and majority size, and examines how the subject agent rationalizes its
conforming behavior. Furthermore, we explore two strategies to mitigate
conformity effects, i.e., developing enhanced personas and implementing a
reflection mechanism. Several interesting findings regarding LLMs' conformity
are derived from empirical results and case studies. We hope that these
insights can pave the way for more robust and ethically-aligned collaborative
AI systems. Our benchmark and code are available at BenchForm.
</summary>
    <author>
      <name>Zhiyuan Weng</name>
    </author>
    <author>
      <name>Guikun Chen</name>
    </author>
    <author>
      <name>Wenguan Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2025. Code: https://github.com/Zhiyuan-Weng/BenchForm</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13351v1</id>
    <updated>2025-01-23T03:28:38Z</updated>
    <published>2025-01-23T03:28:38Z</published>
    <title>50 Shades of Deceptive Patterns: A Unified Taxonomy, Multimodal
  Detection, and Security Implications</title>
    <summary>  Deceptive patterns (DPs) are user interface designs deliberately crafted to
manipulate users into unintended decisions, often by exploiting cognitive
biases for the benefit of companies or services. While numerous studies have
explored ways to identify these deceptive patterns, many existing solutions
require significant human intervention and struggle to keep pace with the
evolving nature of deceptive designs. To address these challenges, we expanded
the deceptive pattern taxonomy from security and privacy perspectives, refining
its categories and scope. We created a comprehensive dataset of deceptive
patterns by integrating existing small-scale datasets with new samples,
resulting in 6,725 images and 10,421 DP instances from mobile apps and
websites. We then developed DPGuard, a novel automatic tool leveraging
commercial multimodal large language models (MLLMs) for deceptive pattern
detection. Experimental results show that DPGuard outperforms state-of-the-art
methods. Finally, we conducted an extensive empirical evaluation on 2,000
popular mobile apps and websites, revealing that 23.61% of mobile screenshots
and 47.27% of website screenshots feature at least one deceptive pattern
instance. Through four unexplored case studies that inform security
implications, we highlight the critical importance of the unified taxonomy in
addressing the growing challenges of Internet deception.
</summary>
    <author>
      <name>Zewei Shi</name>
    </author>
    <author>
      <name>Ruoxi Sun</name>
    </author>
    <author>
      <name>Jieshan Chen</name>
    </author>
    <author>
      <name>Jiamou Sun</name>
    </author>
    <author>
      <name>Minhui Xue</name>
    </author>
    <author>
      <name>Yansong Gao</name>
    </author>
    <author>
      <name>Feng Liu</name>
    </author>
    <author>
      <name>Xingliang Yuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted by The Web Conference 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13344v1</id>
    <updated>2025-01-23T03:05:13Z</updated>
    <published>2025-01-23T03:05:13Z</published>
    <title>Full-Stack Optimized Large Language Models for Lifelong Sequential
  Behavior Comprehension in Recommendation</title>
    <summary>  In this paper, we address the lifelong sequential behavior incomprehension
problem in large language models (LLMs) for recommendation, where LLMs struggle
to extract useful information from long user behavior sequences, even within
their context limits. To tackle this, we propose ReLLaX (Retrieval-enhanced
Large Language models Plus), a framework offering optimization across data,
prompt, and parameter levels. At the data level, we introduce Semantic User
Behavior Retrieval (SUBR) to reduce sequence heterogeneity, making it easier
for LLMs to extract key information. For prompt-level enhancement, we employ
Soft Prompt Augmentation (SPA) to inject collaborative knowledge, aligning item
representations with recommendation tasks and improving LLMs's exploration of
item relationships. Finally, at the parameter level, we propose Component
Fully-interactive LoRA (CFLoRA), which enhances LoRA's expressiveness by
enabling interactions between its components, allowing better capture of
sequential information. Moreover, we present new perspectives to compare
current LoRA-based LLM4Rec methods, i.e. from both a composite and a decomposed
view. We theoretically demonstrate that the ways they employ LoRA for
recommendation are degraded versions of our CFLoRA, with different constraints
on atom component interactions. Extensive experiments on three public datasets
demonstrate ReLLaX's superiority over existing baselines and its ability to
mitigate lifelong sequential behavior incomprehension effectively.
</summary>
    <author>
      <name>Rong Shan</name>
    </author>
    <author>
      <name>Jiachen Zhu</name>
    </author>
    <author>
      <name>Jianghao Lin</name>
    </author>
    <author>
      <name>Chenxu Zhu</name>
    </author>
    <author>
      <name>Bo Chen</name>
    </author>
    <author>
      <name>Ruiming Tang</name>
    </author>
    <author>
      <name>Yong Yu</name>
    </author>
    <author>
      <name>Weinan Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under Review</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13341v1</id>
    <updated>2025-01-23T02:45:35Z</updated>
    <published>2025-01-23T02:45:35Z</published>
    <title>Multi-aspect Knowledge Distillation with Large Language Model</title>
    <summary>  Recent advancements in deep learning have significantly improved performance
on computer vision tasks. Previous image classification methods primarily
modify model architectures or add features, and they optimize models using
cross-entropy loss on class logits. Since they focus on classifying images with
considering class labels, these methods may struggle to learn various
\emph{aspects} of classes (e.g., natural positions and shape changes).
Rethinking the previous approach from a novel view, we propose a multi-aspect
knowledge distillation method using Multimodal Large Language Models (MLLMs).
Our approach involves: 1) querying Large Language Model with multi-aspect
questions relevant to the knowledge we want to transfer to the model, 2)
extracting corresponding logits from MLLM, and 3) expanding the model's output
dimensions to distill these multi-aspect logits. We then apply cross-entropy
loss to class logits and binary cross-entropy loss to multi-aspect logits.
Through our method, the model can learn not only the knowledge about visual
aspects but also the abstract and complex aspects that require a deeper
understanding. We primarily apply our method to image classification, and to
explore the potential for extending our model, we expand it to other tasks,
such as object detection. In all experimental results, our method improves the
performance of the baselines. Additionally, we analyze the effect of
multi-aspect knowledge distillation. These results demonstrate that our method
can transfer knowledge about various aspects to the model and the aspect
knowledge can enhance model performance in computer vision tasks. This paper
demonstrates the great potential of multi-aspect knowledge distillation, and we
believe it offers a promising direction for future research in computer vision
and beyond.
</summary>
    <author>
      <name>Taegyeong Lee</name>
    </author>
    <author>
      <name>Jinsik Bang</name>
    </author>
    <author>
      <name>Soyeong Kwon</name>
    </author>
    <author>
      <name>Taehwan Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13333v1</id>
    <updated>2025-01-23T02:25:44Z</updated>
    <published>2025-01-23T02:25:44Z</published>
    <title>AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to
  Human Feedback</title>
    <summary>  Multi-agent systems must decide which agent is the most appropriate for a
given task. We propose a novel architecture for recommending which LLM agent
out of many should perform a task given a natural language prompt by extending
the Sentence-BERT (SBERT) encoder model. On test data, we are able to achieve a
top-1 accuracy of 92.2% with each classification taking less than 300
milliseconds. In contrast to traditional classification methods, our
architecture is computationally cheap, adaptive to new classes, interpretable,
and controllable with arbitrary metrics through reinforcement learning. By
encoding natural language prompts into sentence embeddings, our model captures
the semantic content relevant to recommending an agent. The distance between
sentence embeddings that belong to the same agent is then minimized through
fine-tuning and aligned to human values through reinforcement learning from
human feedback. This allows the classification of natural language prompts
based on their nearest neighbors by measuring the cosine similarity between
embeddings. This work is made possible through the generation of a synthetic
dataset for agent recommendation, which we have open-sourced to the public
along with the code for AgentRec recommendation system at
https://github.com/joshprk/agentrec.
</summary>
    <author>
      <name>Joshua Park</name>
    </author>
    <author>
      <name>Yongfeng Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figures, preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13331v1</id>
    <updated>2025-01-23T02:20:08Z</updated>
    <published>2025-01-23T02:20:08Z</published>
    <title>Qrazor: Reliable and effortless 4-bit llm quantization by significant
  data razoring</title>
    <summary>  Large-scale language models (LLMs) have demonstrated outstanding performance
in language processing tasks, yet their deployment is often hindered by high
memory demands and computational complexity. Although low-bit quantization
techniques, such as 4-bit quantization, present a potential solution, they
frequently lead to significant accuracy degradation or require substantial
effort for such aggressive quantization approaches. To overcome these
challenges, we introduce QRazor, a reliable and effortless quantization scheme
designed to enable 4-bit quantization for weights, activations, and KV cache in
transformer-based LLMs. The scheme involves two main stages: quantization and
compression. During the quantization stage, weights, activations, and KV cache
values are quantized with wider 8 or 16-bit integers as a basis to achieve
nearly identical accuracy to the original full-precision LLM models, using the
absolute max scaling. Subsequently, all data are compressed to 4-bit using our
proposed significant data razoring (SDR) technique, which retains only the four
most salient bits while discarding the others. Furthermore, we present an
integer-based arithmetic unit dedicated to QRazor, enabling direct
low-precision arithmetic operations without decompressing the SDR data. Despite
the reduced quantization effort, QRazor achieves LLM accuracies better or
comparable to state-of-the-art 4-bit methods. By also validating the hardware
efficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%
reduction in area and power consumption, respectively.
</summary>
    <author>
      <name>Dongyoung Lee</name>
    </author>
    <author>
      <name>Seungkyu Choi</name>
    </author>
    <author>
      <name>Ik Joon Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13331v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13331v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13318v1</id>
    <updated>2025-01-23T02:02:38Z</updated>
    <published>2025-01-23T02:02:38Z</published>
    <title>SplitLLM: Hierarchical Split Learning for Large Language Model over
  Wireless Network</title>
    <summary>  Fine-tuning a large language model (LLM) using the local data of edge users
can enable personalized services and applications. For privacy protection, the
prevalent solution adopts distributed learning for fine-tuning and integrates
low-rank adaptation (LoRA) to reduce users' computational load. However, as the
number of users increases, numerous users simultaneously communicate with the
server, and multiple server-side models concurrently execute on the server,
leading to significant communication congestion and memory pressure. In this
paper, we propose a split learning (SL) scheme for fine-tuning LLM in wireless
networks, which involves one cloud server, a small number of edge servers, and
multiple users. Specifically, the pre-trained model and LoRA adapters are
divided into three parts and deployed across the cloud, edge, and user sides.
The training process follows the sequence of user, edge, and cloud, with
forward and backward propagation achieved by transmitting activation and
gradient. In each round, all edge servers and an equivalent number of users
train in parallel, and only the LoRA adapters are updated. At the end of each
round, all edge-side and user-side LoRA adapters are uploaded to the cloud for
aggregation. Extensive simulation demonstrates that the proposed scheme can
reduce peak memory usage up to 74% compared to the state-of-the-art benchmarks.
</summary>
    <author>
      <name>Songge Zhang</name>
    </author>
    <author>
      <name>Guoliang Cheng</name>
    </author>
    <author>
      <name>Zuguang Li</name>
    </author>
    <author>
      <name>Wen Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages with 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13318v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13318v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13309v1</id>
    <updated>2025-01-23T01:30:14Z</updated>
    <published>2025-01-23T01:30:14Z</published>
    <title>Representing Visualization Insights as a Dense Insight Network</title>
    <summary>  We propose a dense insight network framework to encode the relationships
between automatically generated insights from a complex dashboard based on
their shared characteristics. Our insight network framework includes five
high-level categories of relationships (e.g., type, topic, value, metadata, and
compound scores). The goal of this insight network framework is to provide a
foundation for implementing new insight interpretation and exploration
strategies, including both user-driven and automated approaches. To illustrate
the complexity and flexibility of our framework, we first describe a
visualization playground to directly visualize key network characteristics;
this playground also demonstrates potential interactive capabilities for
decomposing the dense insight network. Then, we discuss a case study
application for ranking insights based on the underlying network
characteristics captured by our framework, before prompting a large language
model to generate a concise, natural language summary. Finally, we reflect on
next steps for leveraging our insight network framework to design and evaluate
new systems.
</summary>
    <author>
      <name>Jane Hoffswell</name>
    </author>
    <author>
      <name>Victor Soares Bursztyn</name>
    </author>
    <author>
      <name>Shunan Guo</name>
    </author>
    <author>
      <name>Jesse Martinez</name>
    </author>
    <author>
      <name>Eunyee Koh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Currently Under Review</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13306v1</id>
    <updated>2025-01-23T01:27:46Z</updated>
    <published>2025-01-23T01:27:46Z</published>
    <title>OSUM: Advancing Open Speech Understanding Models with Limited Resources
  in Academia</title>
    <summary>  Large Language Models (LLMs) have made significant progress in various
downstream tasks, inspiring the development of Speech Understanding Language
Models (SULMs) to enable comprehensive speech-based interactions. However, most
advanced SULMs are developed by the industry, leveraging large-scale datasets
and computational resources that are not readily available to the academic
community. Moreover, the lack of transparency in training details creates
additional barriers to further innovation. In this study, we present OSUM, an
Open Speech Understanding Model designed to explore the potential of training
SLUMs under constrained academic resources. The OSUM model combines a Whisper
encoder with a Qwen2 LLM and supports a wide range of speech tasks, including
speech recognition (ASR), speech recognition with timestamps (SRWT), vocal
event detection (VED), speech emotion recognition (SER), speaking style
recognition (SSR), speaker gender classification (SGC), speaker age prediction
(SAP), and speech-to-text chat (STTC). By employing an ASR+X training strategy,
OSUM achieves efficient and stable multi-task training by simultaneously
optimizing ASR alongside target tasks. Beyond delivering strong performance,
OSUM emphasizes transparency by providing openly available data preparation and
training methodologies, offering valuable insights and practical guidance for
the academic community. By doing so, we aim to accelerate research and
innovation in advanced SULM technologies.
</summary>
    <author>
      <name>Xuelong Geng</name>
    </author>
    <author>
      <name>Kun Wei</name>
    </author>
    <author>
      <name>Qijie Shao</name>
    </author>
    <author>
      <name>Shuiyun Liu</name>
    </author>
    <author>
      <name>Zhennan Lin</name>
    </author>
    <author>
      <name>Zhixian Zhao</name>
    </author>
    <author>
      <name>Guojian Li</name>
    </author>
    <author>
      <name>Wenjie Tian</name>
    </author>
    <author>
      <name>Peikun Chen</name>
    </author>
    <author>
      <name>Yangze Li</name>
    </author>
    <author>
      <name>Pengcheng Guo</name>
    </author>
    <author>
      <name>Mingchen Shao</name>
    </author>
    <author>
      <name>Shuiyuan Wang</name>
    </author>
    <author>
      <name>Yuang Cao</name>
    </author>
    <author>
      <name>Chengyou Wang</name>
    </author>
    <author>
      <name>Tianyi Xu</name>
    </author>
    <author>
      <name>Yuhang Dai</name>
    </author>
    <author>
      <name>Xinfa Zhu</name>
    </author>
    <author>
      <name>Yue Li</name>
    </author>
    <author>
      <name>Li Zhang</name>
    </author>
    <author>
      <name>Lei Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">OSUM Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13302v1</id>
    <updated>2025-01-23T01:04:00Z</updated>
    <published>2025-01-23T01:04:00Z</published>
    <title>Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI
  Safety Moderation Classifiers</title>
    <summary>  AI Safety Moderation (ASM) classifiers are designed to moderate content on
social media platforms and to serve as guardrails that prevent Large Language
Models (LLMs) from being fine-tuned on unsafe inputs. Owing to their potential
for disparate impact, it is crucial to ensure that these classifiers: (1) do
not unfairly classify content belonging to users from minority groups as unsafe
compared to those from majority groups and (2) that their behavior remains
robust and consistent across similar inputs. In this work, we thus examine the
fairness and robustness of four widely-used, closed-source ASM classifiers:
OpenAI Moderation API, Perspective API, Google Cloud Natural Language (GCNL)
API, and Clarifai API. We assess fairness using metrics such as demographic
parity and conditional statistical parity, comparing their performance against
ASM models and a fair-only baseline. Additionally, we analyze robustness by
testing the classifiers' sensitivity to small and natural input perturbations.
Our findings reveal potential fairness and robustness gaps, highlighting the
need to mitigate these issues in future versions of these models.
</summary>
    <author>
      <name>Akshit Achara</name>
    </author>
    <author>
      <name>Anshuman Chhabra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to NAACL 2025 Main Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13299v1</id>
    <updated>2025-01-23T01:01:05Z</updated>
    <published>2025-01-23T01:01:05Z</published>
    <title>Hypothesis Generation for Materials Discovery and Design Using
  Goal-Driven and Constraint-Guided LLM Agents</title>
    <summary>  Materials discovery and design are essential for advancing technology across
various industries by enabling the development of application-specific
materials. Recent research has leveraged Large Language Models (LLMs) to
accelerate this process. We explore the potential of LLMs to generate viable
hypotheses that, once validated, can expedite materials discovery.
Collaborating with materials science experts, we curated a novel dataset from
recent journal publications, featuring real-world goals, constraints, and
methods for designing real-world applications. Using this dataset, we test
LLM-based agents that generate hypotheses for achieving given goals under
specific constraints. To assess the relevance and quality of these hypotheses,
we propose a novel scalable evaluation metric that emulates the process a
materials scientist would use to evaluate a hypothesis critically. Our curated
dataset, proposed method, and evaluation framework aim to advance future
research in accelerating materials discovery and design with LLMs.
</summary>
    <author>
      <name>Shrinidhi Kumbhar</name>
    </author>
    <author>
      <name>Venkatesh Mishra</name>
    </author>
    <author>
      <name>Kevin Coutinho</name>
    </author>
    <author>
      <name>Divij Handa</name>
    </author>
    <author>
      <name>Ashif Iquebal</name>
    </author>
    <author>
      <name>Chitta Baral</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in NAACL 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13297v1</id>
    <updated>2025-01-23T00:50:33Z</updated>
    <published>2025-01-23T00:50:33Z</published>
    <title>RAMQA: A Unified Framework for Retrieval-Augmented Multi-Modal Question
  Answering</title>
    <summary>  Multi-modal retrieval-augmented Question Answering (MRAQA), integrating text
and images, has gained significant attention in information retrieval (IR) and
natural language processing (NLP). Traditional ranking methods rely on small
encoder-based language models, which are incompatible with modern decoder-based
generative large language models (LLMs) that have advanced various NLP tasks.
To bridge this gap, we propose RAMQA, a unified framework combining
learning-to-rank methods with generative permutation-enhanced ranking
techniques. We first train a pointwise multi-modal ranker using LLaVA as the
backbone. Then, we apply instruction tuning to train a LLaMA model for
re-ranking the top-k documents using an innovative autoregressive multi-task
learning approach. Our generative ranking model generates re-ranked document
IDs and specific answers from document candidates in various permutations.
Experiments on two MRAQA benchmarks, WebQA and MultiModalQA, show significant
improvements over strong baselines, highlighting the effectiveness of our
approach. Code and data are available at: https://github.com/TonyBY/RAMQA
</summary>
    <author>
      <name>Yang Bai</name>
    </author>
    <author>
      <name>Christan Earl Grant</name>
    </author>
    <author>
      <name>Daisy Zhe Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by NAACL 2025 Findings</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13284v1</id>
    <updated>2025-01-23T00:20:38Z</updated>
    <published>2025-01-23T00:20:38Z</published>
    <title>Toyteller: AI-powered Visual Storytelling Through Toy-Playing with
  Character Symbols</title>
    <summary>  We introduce Toyteller, an AI-powered storytelling system where users
generate a mix of story text and visuals by directly manipulating character
symbols like they are toy-playing. Anthropomorphized symbol motions can convey
rich and nuanced social interactions; Toyteller leverages these motions (1) to
let users steer story text generation and (2) as a visual output format that
accompanies story text. We enabled motion-steered text generation and
text-steered motion generation by mapping motions and text onto a shared
semantic space so that large language models and motion generation models can
use it as a translational layer. Technical evaluations showed that Toyteller
outperforms a competitive baseline, GPT-4o. Our user study identified that
toy-playing helps express intentions difficult to verbalize. However, only
motions could not express all user intentions, suggesting combining it with
other modalities like language. We discuss the design space of toy-playing
interactions and implications for technical HCI research on human-AI
interaction.
</summary>
    <author>
      <name>John Joon Young Chung</name>
    </author>
    <author>
      <name>Melissa Roemmele</name>
    </author>
    <author>
      <name>Max Kreminski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3706598.3713435</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3706598.3713435" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CHI2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13284v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13284v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13264v1</id>
    <updated>2025-01-22T22:59:19Z</updated>
    <published>2025-01-22T22:59:19Z</published>
    <title>RAG-Reward: Optimizing RAG with Reward Modeling and RLHF</title>
    <summary>  Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs)
with relevant and up-to-date knowledge, improving their ability to answer
knowledge-intensive questions. It has been shown to enhance both generation
quality and trustworthiness. While numerous works have focused on improving
retrieval, generation, and evaluation, the role of reward models in
reinforcement learning for optimizing RAG and establishing automated
benchmarking pipelines remains underexplored. In this paper, we introduce
\textbf{RAG-Reward}, a dataset designed to enable \textit{hallucination-free,
comprehensive, reliable, and efficient RAG}. We define four key metrics for
assessing generation quality and develop an automated annotation pipeline that
leverages multiple LLMs to generate outputs across diverse RAG scenarios.
GPT-4o is used to evaluate and construct preference data. Using
\textbf{RAG-Reward}, we train reward models and apply reinforcement learning
with human feedback (RLHF) to improve LLMs' effectiveness in RAG. Experimental
results show that our reward model achieves state-of-the-art performance on a
held-out test set, demonstrating both the effectiveness of our approach and the
quality of our dataset. Furthermore, the improved generation quality of the
trained policy model highlights the feasibility of using RLHF to enhance RAG
pipelines.
</summary>
    <author>
      <name>Hanning Zhang</name>
    </author>
    <author>
      <name>Juntong Song</name>
    </author>
    <author>
      <name>Juno Zhu</name>
    </author>
    <author>
      <name>Yuanhao Wu</name>
    </author>
    <author>
      <name>Tong Zhang</name>
    </author>
    <author>
      <name>Cheng Niu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint, work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13261v1</id>
    <updated>2025-01-22T22:49:27Z</updated>
    <published>2025-01-22T22:49:27Z</published>
    <title>Exploring GPT's Ability as a Judge in Music Understanding</title>
    <summary>  Recent progress in text-based Large Language Models (LLMs) and their extended
ability to process multi-modal sensory data have led us to explore their
applicability in addressing music information retrieval (MIR) challenges. In
this paper, we use a systematic prompt engineering approach for LLMs to solve
MIR problems. We convert the music data to symbolic inputs and evaluate LLMs'
ability in detecting annotation errors in three key MIR tasks: beat tracking,
chord extraction, and key estimation. A concept augmentation method is proposed
to evaluate LLMs' music reasoning consistency with the provided music concepts
in the prompts. Our experiments tested the MIR capabilities of Generative
Pre-trained Transformers (GPT). Results show that GPT has an error detection
accuracy of 65.20%, 64.80%, and 59.72% in beat tracking, chord extraction, and
key estimation tasks, respectively, all exceeding the random baseline.
Moreover, we observe a positive correlation between GPT's error finding
accuracy and the amount of concept information provided. The current findings
based on symbolic music input provide a solid ground for future LLM-based MIR
research.
</summary>
    <author>
      <name>Kun Fang</name>
    </author>
    <author>
      <name>Ziyu Wang</name>
    </author>
    <author>
      <name>Gus Xia</name>
    </author>
    <author>
      <name>Ichiro Fujinaga</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13256v1</id>
    <updated>2025-01-22T22:25:10Z</updated>
    <published>2025-01-22T22:25:10Z</published>
    <title>Bypassing Array Canaries via Autonomous Function Call Resolution</title>
    <summary>  We observed the Array Canary, a novel JavaScript anti-analysis technique
currently exploited in-the-wild by the Phishing-as-a-Service framework Darcula.
The Array Canary appears to be an advanced form of the array shuffling
techniques employed by the Emotet JavaScript downloader. In practice, a series
of Array Canaries are set within a string array and if modified will cause the
program to endlessly loop. In this paper, we demonstrate how an Array Canary
works and discuss Autonomous Function Call Resolution (AFCR), which is a method
we created to bypass Array Canaries. We also introduce Arphsy, a
proof-of-concept for AFCR designed to guide Large Language Models and security
researchers in the deobfuscation of "canaried" JavaScript code. We accomplish
this by (i) Finding and extracting all Immediately Invoked Function Expressions
from a canaried file, (ii) parsing the file's Abstract Syntax Tree for any
function that does not implement imported function calls, (iii) identifying the
most reassigned variable and its corresponding function body, (iv) calculating
the length of the largest string array and uses it to determine the offset
values within the canaried file, (v) aggregating all the previously identified
functions into a single file, and (vi) appending driver code into the verified
file and using it to deobfuscate the canaried file.
</summary>
    <author>
      <name>Nathaniel Oh</name>
    </author>
    <author>
      <name>Paul Attie</name>
    </author>
    <author>
      <name>Anas Obeidat</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13106v2</id>
    <updated>2025-01-23T14:41:06Z</updated>
    <published>2025-01-22T18:59:46Z</published>
    <title>VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video
  Understanding</title>
    <summary>  In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation
model for image and video understanding. The core design philosophy of
VideoLLaMA3 is vision-centric. The meaning of "vision-centric" is two-fold: the
vision-centric training paradigm and vision-centric framework design. The key
insight of our vision-centric training paradigm is that high-quality image-text
data is crucial for both image and video understanding. Instead of preparing
massive video-text datasets, we focus on constructing large-scale and
high-quality image-text datasets. VideoLLaMA3 has four training stages: 1)
Vision Encoder Adaptation, which enables vision encoder to accept images of
variable resolutions as input; 2) Vision-Language Alignment, which jointly
tunes the vision encoder, projector, and LLM with large-scale image-text data
covering multiple types (including scene images, documents, charts) as well as
text-only data. 3) Multi-task Fine-tuning, which incorporates image-text SFT
data for downstream tasks and video-text data to establish a foundation for
video understanding. 4) Video-centric Fine-tuning, which further improves the
model's capability in video understanding. As for the framework design, to
better capture fine-grained details in images, the pretrained vision encoder is
adapted to encode images of varying sizes into vision tokens with corresponding
numbers, rather than a fixed number of tokens. For video inputs, we reduce the
number of vision tokens according to their similarity so that the
representation of videos will be more precise and compact. Benefit from
vision-centric designs, VideoLLaMA3 achieves compelling performances in both
image and video understanding benchmarks.
</summary>
    <author>
      <name>Boqiang Zhang</name>
    </author>
    <author>
      <name>Kehan Li</name>
    </author>
    <author>
      <name>Zesen Cheng</name>
    </author>
    <author>
      <name>Zhiqiang Hu</name>
    </author>
    <author>
      <name>Yuqian Yuan</name>
    </author>
    <author>
      <name>Guanzheng Chen</name>
    </author>
    <author>
      <name>Sicong Leng</name>
    </author>
    <author>
      <name>Yuming Jiang</name>
    </author>
    <author>
      <name>Hang Zhang</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Peng Jin</name>
    </author>
    <author>
      <name>Wenqi Zhang</name>
    </author>
    <author>
      <name>Fan Wang</name>
    </author>
    <author>
      <name>Lidong Bing</name>
    </author>
    <author>
      <name>Deli Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BZ, KL, ZC, ZH, YY, GC, SL, YJ, HZ, and XL contributed equally to
  this project. Code: https://github.com/DAMO-NLP-SG/VideoLLaMA3</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13106v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13106v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13080v1</id>
    <updated>2025-01-22T18:40:57Z</updated>
    <published>2025-01-22T18:40:57Z</published>
    <title>Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through
  Chain-of-Thought Fine-Tuning and Alignment</title>
    <summary>  Large Language Models (LLMs) have demonstrated powerful capabilities that
render them valuable in different applications, including conversational AI
products. It is paramount to ensure the security and reliability of these
products by mitigating their vulnerabilities towards malicious user
interactions, which can lead to the exposure of great risks and reputational
repercussions. In this work, we present a comprehensive study on the efficacy
of fine-tuning and aligning Chain-of-Thought (CoT) responses of different LLMs
that serve as input moderation guardrails. We systematically explore various
tuning methods by leveraging a small set of training data to adapt these models
as proxy defense mechanisms to detect malicious inputs and provide a reasoning
for their verdicts, thereby preventing the exploitation of conversational
agents. We rigorously evaluate the efficacy and robustness of different tuning
strategies to generalize across diverse adversarial and malicious query types.
Our experimental results outline the potential of alignment processes tailored
to a varied range of harmful input queries, even with constrained data
resources. These techniques significantly enhance the safety of conversational
AI systems and provide a feasible framework for deploying more secure and
trustworthy AI-driven interactions.
</summary>
    <author>
      <name>Melissa Kazemi Rad</name>
    </author>
    <author>
      <name>Huy Nghiem</name>
    </author>
    <author>
      <name>Andy Luo</name>
    </author>
    <author>
      <name>Sahil Wadhwa</name>
    </author>
    <author>
      <name>Mohammad Sorower</name>
    </author>
    <author>
      <name>Stephen Rawls</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13042v1</id>
    <updated>2025-01-22T17:44:01Z</updated>
    <published>2025-01-22T17:44:01Z</published>
    <title>Does Table Source Matter? Benchmarking and Improving Multimodal
  Scientific Table Understanding and Reasoning</title>
    <summary>  Recent large language models (LLMs) have advanced table understanding
capabilities but rely on converting tables into text sequences. While
multimodal large language models (MLLMs) enable direct visual processing, they
face limitations in handling scientific tables due to fixed input image
resolutions and insufficient numerical reasoning capabilities. We present a
comprehensive framework for multimodal scientific table understanding and
reasoning with dynamic input image resolutions. Our framework consists of three
key components: (1) MMSci-Pre, a domain-specific table structure learning
dataset of 52K scientific table structure recognition samples, (2) MMSci-Ins,
an instruction tuning dataset with 12K samples across three table-based tasks,
and (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically
designed to evaluate numerical reasoning capabilities. Extensive experiments
demonstrate that our domain-specific approach with 52K scientific table images
achieves superior performance compared to 150K general-domain tables,
highlighting the importance of data quality over quantity. Our proposed
table-based MLLMs with dynamic input resolutions show significant improvements
in both general table understanding and numerical reasoning capabilities, with
strong generalisation to held-out datasets. Our code and data are publicly
available at https://github.com/Bernard-Yang/MMSci_Table.
</summary>
    <author>
      <name>Bohao Yang</name>
    </author>
    <author>
      <name>Yingji Zhang</name>
    </author>
    <author>
      <name>Dong Liu</name>
    </author>
    <author>
      <name>André Freitas</name>
    </author>
    <author>
      <name>Chenghua Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13011v1</id>
    <updated>2025-01-22T16:53:08Z</updated>
    <published>2025-01-22T16:53:08Z</published>
    <title>MONA: Myopic Optimization with Non-myopic Approval Can Mitigate
  Multi-step Reward Hacking</title>
    <summary>  Future advanced AI systems may learn sophisticated strategies through
reinforcement learning (RL) that humans cannot understand well enough to safely
evaluate. We propose a training method which avoids agents learning undesired
multi-step plans that receive high reward (multi-step "reward hacks") even if
humans are not able to detect that the behaviour is undesired. The method,
Myopic Optimization with Non-myopic Approval (MONA), works by combining
short-sighted optimization with far-sighted reward. We demonstrate that MONA
can prevent multi-step reward hacking that ordinary RL causes, even without
being able to detect the reward hacking and without any extra information that
ordinary RL does not get access to. We study MONA empirically in three settings
which model different misalignment failure modes including 2-step environments
with LLMs representing delegated oversight and encoded reasoning and
longer-horizon gridworld environments representing sensor tampering.
</summary>
    <author>
      <name>Sebastian Farquhar</name>
    </author>
    <author>
      <name>Vikrant Varma</name>
    </author>
    <author>
      <name>David Lindner</name>
    </author>
    <author>
      <name>David Elson</name>
    </author>
    <author>
      <name>Caleb Biddulph</name>
    </author>
    <author>
      <name>Ian Goodfellow</name>
    </author>
    <author>
      <name>Rohin Shah</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13007v1</id>
    <updated>2025-01-22T16:49:37Z</updated>
    <published>2025-01-22T16:49:37Z</published>
    <title>Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament</title>
    <summary>  Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large
Language Models (LLMs), relies on reward models to select the best candidate
solution from multiple generations. However, traditional reward models often
assign arbitrary and inconsistent scores, limiting their effectiveness. To
address this, we propose a Pairwise Reward Model (Pairwise RM) combined with a
knockout tournament for BoN sampling. Instead of assigning absolute scores,
given one math problem, Pairwise RM evaluates two candidate solutions'
correctness simultaneously. This approach eliminates the need for arbitrary
scoring and enables cross-validation of solutions through parallel comparison.
In the knockout tournament, Pairwise RM conducts pairwise comparisons between
candidate solutions and eliminates the incorrect ones iteratively. We construct
\ourdataset, a large-scale dataset of 443K pairwise comparisons derived from
NumiaMath and annotated using \texttt{gemini-1.5-flash}, and train the Pairwise
RM via supervised fine-tuning. Experiments on MATH-500 and the Olympiad Bench
demonstrate significant improvements over traditional discriminative reward
models. And a 40\% to 60\% relative improvement is achieved on the top 50\%
challenging problems.
</summary>
    <author>
      <name>Yantao Liu</name>
    </author>
    <author>
      <name>Zijun Yao</name>
    </author>
    <author>
      <name>Rui Min</name>
    </author>
    <author>
      <name>Yixin Cao</name>
    </author>
    <author>
      <name>Lei Hou</name>
    </author>
    <author>
      <name>Juanzi Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in progress work</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12988v1</id>
    <updated>2025-01-22T16:20:47Z</updated>
    <published>2025-01-22T16:20:47Z</published>
    <title>Large Language Model-Based Semantic Communication System for Image
  Transmission</title>
    <summary>  The remarkable success of Large Language Models (LLMs) in understanding and
generating various data types, such as images and text, has demonstrated their
ability to process and extract semantic information across diverse domains.
This transformative capability lays the foundation for semantic communications,
enabling highly efficient and intelligent communication systems. In this work,
we present a novel OFDM-based semantic communication framework for image
transmission. We propose an innovative semantic encoder design that leverages
the ability of LLMs to extract the meaning of transmitted data rather than
focusing on its raw representation. On the receiver side, we design an
LLM-based semantic decoder capable of comprehending context and generating the
most appropriate representation to fit the given context. We evaluate our
proposed system under different scenarios, including Urban Macro-cell
environments with varying speed ranges. The evaluation metrics demonstrate that
our proposed system reduces the data size 4250 times, while achieving a higher
data rate compared to conventional communication methods. This approach offers
a robust and scalable solution to unlock the full potential of 6G connectivity.
</summary>
    <author>
      <name>Soheyb Ribouh</name>
    </author>
    <author>
      <name>Osama Saleem</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12983v1</id>
    <updated>2025-01-22T16:12:38Z</updated>
    <published>2025-01-22T16:12:38Z</published>
    <title>LLM4WM: Adapting LLM for Wireless Multi-Tasking</title>
    <summary>  The wireless channel is fundamental to communication, encompassing numerous
tasks collectively referred to as channel-associated tasks. These tasks can
leverage joint learning based on channel characteristics to share
representations and enhance system design. To capitalize on this advantage,
LLM4WM is proposed--a large language model (LLM) multi-task fine-tuning
framework specifically tailored for channel-associated tasks. This framework
utilizes a Mixture of Experts with Low-Rank Adaptation (MoE-LoRA) approach for
multi-task fine-tuning, enabling the transfer of the pre-trained LLM's general
knowledge to these tasks. Given the unique characteristics of wireless channel
data, preprocessing modules, adapter modules, and multi-task output layers are
designed to align the channel data with the LLM's semantic feature space.
Experiments on a channel-associated multi-task dataset demonstrate that LLM4WM
outperforms existing methodologies in both full-sample and few-shot
evaluations, owing to its robust multi-task joint modeling and transfer
learning capabilities.
</summary>
    <author>
      <name>Xuanyu Liu</name>
    </author>
    <author>
      <name>Shijian Gao</name>
    </author>
    <author>
      <name>Boxun Liu</name>
    </author>
    <author>
      <name>Xiang Cheng</name>
    </author>
    <author>
      <name>Liuqing Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12983v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12983v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12980v1</id>
    <updated>2025-01-22T16:07:24Z</updated>
    <published>2025-01-22T16:07:24Z</published>
    <title>Implicit Causality-biases in humans and LLMs as a tool for benchmarking
  LLM discourse capabilities</title>
    <summary>  In this paper, we compare data generated with mono- and multilingual LLMs
spanning a range of model sizes with data provided by human participants in an
experimental setting investigating well-established discourse biases. Beyond
the comparison as such, we aim to develop a benchmark to assess the
capabilities of LLMs with discourse biases as a robust proxy for more general
discourse understanding capabilities. More specifically, we investigated
Implicit Causality verbs, for which psycholinguistic research has found
participants to display biases with regard to three phenomena:\ the
establishment of (i) coreference relations (Experiment 1), (ii) coherence
relations (Experiment 2), and (iii) the use of particular referring expressions
(Experiments 3 and 4). With regard to coreference biases we found only the
largest monolingual LLM (German Bloom 6.4B) to display more human-like biases.
For coherence relation, no LLM displayed the explanation bias usually found for
humans. For referring expressions, all LLMs displayed a preference for
referring to subject arguments with simpler forms than to objects. However, no
bias effect on referring expression was found, as opposed to recent studies
investigating human biases.
</summary>
    <author>
      <name>Florian Kankowski</name>
    </author>
    <author>
      <name>Torgrim Solstad</name>
    </author>
    <author>
      <name>Sina Zarriess</name>
    </author>
    <author>
      <name>Oliver Bott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.12980v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12980v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12975v1</id>
    <updated>2025-01-22T15:59:44Z</updated>
    <published>2025-01-22T15:59:44Z</published>
    <title>OnionEval: An Unified Evaluation of Fact-conflicting Hallucination for
  Small-Large Language Models</title>
    <summary>  Large Language Models (LLMs) are highly capable but require significant
computational resources for both training and inference. Within the LLM family,
smaller models (those with fewer than 10 billion parameters) also perform well
across various tasks. However, these smaller models share similar limitations
to their larger counterparts, including the tendency to hallucinate. Despite
the existence of many benchmarks to evaluate hallucination in LLMs, few have
specifically focused on small LLMs (SLLMs). Additionally, SLLMs show widely
varying performance across different benchmarks. In this paper, we introduce
OnionEval, a multi-layer structured framework with a specific metric called the
context-influence score (CI), designed to effectively assess the
fact-conflicting hallucination tendencies of small LLMs across different
contextual levels. Our experimental results reveal a key feature of SLLMs: they
excel in factual analysis but face challenges with context reasoning. Further
investigation shows that a simple Chain-of-Thought strategy can significantly
reduce these limitations, improving the practical usefulness of SLLMs in
real-world applications.
</summary>
    <author>
      <name>Chongren Sun</name>
    </author>
    <author>
      <name>Yuran Li</name>
    </author>
    <author>
      <name>Di Wu</name>
    </author>
    <author>
      <name>Benoit Boulet</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12975v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12975v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12972v1</id>
    <updated>2025-01-22T15:57:29Z</updated>
    <published>2025-01-22T15:57:29Z</published>
    <title>Accessible Smart Contracts Verification: Synthesizing Formal Models with
  Tamed LLMs</title>
    <summary>  When blockchain systems are said to be trustless, what this really means is
that all the trust is put into software. Thus, there are strong incentives to
ensure blockchain software is correct -- vulnerabilities here cost millions and
break businesses. One of the most powerful ways of establishing software
correctness is by using formal methods. Approaches based on formal methods,
however, induce a significant overhead in terms of time and expertise required
to successfully employ them. Our work addresses this critical disadvantage by
automating the creation of a formal model -- a mathematical abstraction of the
software system -- which is often a core task when employing formal methods. We
perform model synthesis in three phases: we first transpile the code into model
stubs; then we "fill in the blanks" using a large language model (LLM);
finally, we iteratively repair the generated model, on both syntactical and
semantical level. In this way, we significantly reduce the amount of time
necessary to create formal models and increase accessibility of valuable
software verification methods that rely on them. The practical context of our
work was reducing the time-to-value of using formal models for correctness
audits of smart contracts.
</summary>
    <author>
      <name>Jan Corazza</name>
    </author>
    <author>
      <name>Ivan Gavran</name>
    </author>
    <author>
      <name>Gabriela Moreira</name>
    </author>
    <author>
      <name>Daniel Neider</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12972v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12972v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12962v1</id>
    <updated>2025-01-22T15:38:09Z</updated>
    <published>2025-01-22T15:38:09Z</published>
    <title>It's complicated. The relationship of algorithmic fairness and
  non-discrimination regulations in the EU AI Act</title>
    <summary>  What constitutes a fair decision? This question is not only difficult for
humans but becomes more challenging when Artificial Intelligence (AI) models
are used. In light of discriminatory algorithmic behaviors, the EU has recently
passed the AI Act, which mandates specific rules for AI models, incorporating
both traditional legal non-discrimination regulations and machine learning
based algorithmic fairness concepts. This paper aims to bridge these two
different concepts in the AI Act through: First a high-level introduction of
both concepts targeting legal and computer science-oriented scholars, and
second an in-depth analysis of the AI Act's relationship between legal
non-discrimination regulations and algorithmic fairness. Our analysis reveals
three key findings: (1.), most non-discrimination regulations target only
high-risk AI systems. (2.), the regulation of high-risk systems encompasses
both data input requirements and output monitoring, though these regulations
are often inconsistent and raise questions of computational feasibility. (3.)
Regulations for General Purpose AI Models, such as Large Language Models that
are not simultaneously classified as high-risk systems, currently lack
specificity compared to other regulations. Based on these findings, we
recommend developing more specific auditing and testing methodologies for AI
systems. This paper aims to serve as a foundation for future interdisciplinary
collaboration between legal scholars and computer science-oriented machine
learning researchers studying discrimination in AI systems.
</summary>
    <author>
      <name>Kristof Meding</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12962v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12962v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12959v1</id>
    <updated>2025-01-22T15:33:17Z</updated>
    <published>2025-01-22T15:33:17Z</published>
    <title>Efficient Prompt Compression with Evaluator Heads for Long-Context
  Transformer Inference</title>
    <summary>  Although applications involving long-context inputs are crucial for the
effective utilization of large language models (LLMs), they also result in
increased computational costs and reduced performance. To address this
challenge, we propose an efficient, training-free prompt compression method
that retains key information within compressed prompts. We identify specific
attention heads in transformer-based LLMs, which we designate as evaluator
heads, that are capable of selecting tokens in long inputs that are most
significant for inference. Building on this discovery, we develop EHPC, an
Evaluator Head-based Prompt Compression method, which enables LLMs to rapidly
"skim through" input prompts by leveraging only the first few layers with
evaluator heads during the pre-filling stage, subsequently passing only the
important tokens to the model for inference. EHPC achieves state-of-the-art
results across two mainstream benchmarks: prompt compression and long-context
inference acceleration. Consequently, it effectively reduces the complexity and
costs associated with commercial API calls. We further demonstrate that EHPC
attains competitive results compared to key-value cache-based acceleration
methods, thereby highlighting its potential to enhance the efficiency of LLMs
for long-context tasks.
</summary>
    <author>
      <name>Weizhi Fei</name>
    </author>
    <author>
      <name>Xueyan Niu</name>
    </author>
    <author>
      <name>Guoqing Xie</name>
    </author>
    <author>
      <name>Yingqing Liu</name>
    </author>
    <author>
      <name>Bo Bai</name>
    </author>
    <author>
      <name>Wei Han</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12956v1</id>
    <updated>2025-01-22T15:29:09Z</updated>
    <published>2025-01-22T15:29:09Z</published>
    <title>GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models</title>
    <summary>  Large Language Models (LLMs) face significant deployment challenges due to
their substantial resource requirements. While low-bit quantized weights can
reduce memory usage and improve inference efficiency, current hardware lacks
native support for mixed-precision General Matrix Multiplication (mpGEMM),
resulting in inefficient dequantization-based implementations. Moreover,
uniform quantization methods often fail to capture weight distributions
adequately, leading to performance degradation. We propose GANQ (GPU-Adaptive
Non-Uniform Quantization), a layer-wise post-training non-uniform quantization
framework optimized for hardware-efficient lookup table-based mpGEMM. GANQ
achieves superior quantization performance by utilizing a training-free,
GPU-adaptive optimization algorithm to efficiently reduce layer-wise
quantization errors. Extensive experiments demonstrate GANQ's ability to reduce
the perplexity gap from the FP16 baseline compared to state-of-the-art methods
for both 3-bit and 4-bit quantization. Furthermore, when deployed on a single
NVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\times$ speedup
over the baseline, advancing memory and inference efficiency in LLM deployment.
</summary>
    <author>
      <name>Pengxiang Zhao</name>
    </author>
    <author>
      <name>Xiaoming Yuan</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12948v1</id>
    <updated>2025-01-22T15:19:35Z</updated>
    <published>2025-01-22T15:19:35Z</published>
    <title>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via
  Reinforcement Learning</title>
    <summary>  We introduce our first-generation reasoning models, DeepSeek-R1-Zero and
DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement
learning (RL) without supervised fine-tuning (SFT) as a preliminary step,
demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero
naturally emerges with numerous powerful and intriguing reasoning behaviors.
However, it encounters challenges such as poor readability, and language
mixing. To address these issues and further enhance reasoning performance, we
introduce DeepSeek-R1, which incorporates multi-stage training and cold-start
data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217
on reasoning tasks. To support the research community, we open-source
DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B,
70B) distilled from DeepSeek-R1 based on Qwen and Llama.
</summary>
    <author>
      <name> DeepSeek-AI</name>
    </author>
    <author>
      <name>Daya Guo</name>
    </author>
    <author>
      <name>Dejian Yang</name>
    </author>
    <author>
      <name>Haowei Zhang</name>
    </author>
    <author>
      <name>Junxiao Song</name>
    </author>
    <author>
      <name>Ruoyu Zhang</name>
    </author>
    <author>
      <name>Runxin Xu</name>
    </author>
    <author>
      <name>Qihao Zhu</name>
    </author>
    <author>
      <name>Shirong Ma</name>
    </author>
    <author>
      <name>Peiyi Wang</name>
    </author>
    <author>
      <name>Xiao Bi</name>
    </author>
    <author>
      <name>Xiaokang Zhang</name>
    </author>
    <author>
      <name>Xingkai Yu</name>
    </author>
    <author>
      <name>Yu Wu</name>
    </author>
    <author>
      <name>Z. F. Wu</name>
    </author>
    <author>
      <name>Zhibin Gou</name>
    </author>
    <author>
      <name>Zhihong Shao</name>
    </author>
    <author>
      <name>Zhuoshu Li</name>
    </author>
    <author>
      <name>Ziyi Gao</name>
    </author>
    <author>
      <name>Aixin Liu</name>
    </author>
    <author>
      <name>Bing Xue</name>
    </author>
    <author>
      <name>Bingxuan Wang</name>
    </author>
    <author>
      <name>Bochao Wu</name>
    </author>
    <author>
      <name>Bei Feng</name>
    </author>
    <author>
      <name>Chengda Lu</name>
    </author>
    <author>
      <name>Chenggang Zhao</name>
    </author>
    <author>
      <name>Chengqi Deng</name>
    </author>
    <author>
      <name>Chenyu Zhang</name>
    </author>
    <author>
      <name>Chong Ruan</name>
    </author>
    <author>
      <name>Damai Dai</name>
    </author>
    <author>
      <name>Deli Chen</name>
    </author>
    <author>
      <name>Dongjie Ji</name>
    </author>
    <author>
      <name>Erhang Li</name>
    </author>
    <author>
      <name>Fangyun Lin</name>
    </author>
    <author>
      <name>Fucong Dai</name>
    </author>
    <author>
      <name>Fuli Luo</name>
    </author>
    <author>
      <name>Guangbo Hao</name>
    </author>
    <author>
      <name>Guanting Chen</name>
    </author>
    <author>
      <name>Guowei Li</name>
    </author>
    <author>
      <name>H. Zhang</name>
    </author>
    <author>
      <name>Han Bao</name>
    </author>
    <author>
      <name>Hanwei Xu</name>
    </author>
    <author>
      <name>Haocheng Wang</name>
    </author>
    <author>
      <name>Honghui Ding</name>
    </author>
    <author>
      <name>Huajian Xin</name>
    </author>
    <author>
      <name>Huazuo Gao</name>
    </author>
    <author>
      <name>Hui Qu</name>
    </author>
    <author>
      <name>Hui Li</name>
    </author>
    <author>
      <name>Jianzhong Guo</name>
    </author>
    <author>
      <name>Jiashi Li</name>
    </author>
    <author>
      <name>Jiawei Wang</name>
    </author>
    <author>
      <name>Jingchang Chen</name>
    </author>
    <author>
      <name>Jingyang Yuan</name>
    </author>
    <author>
      <name>Junjie Qiu</name>
    </author>
    <author>
      <name>Junlong Li</name>
    </author>
    <author>
      <name>J. L. Cai</name>
    </author>
    <author>
      <name>Jiaqi Ni</name>
    </author>
    <author>
      <name>Jian Liang</name>
    </author>
    <author>
      <name>Jin Chen</name>
    </author>
    <author>
      <name>Kai Dong</name>
    </author>
    <author>
      <name>Kai Hu</name>
    </author>
    <author>
      <name>Kaige Gao</name>
    </author>
    <author>
      <name>Kang Guan</name>
    </author>
    <author>
      <name>Kexin Huang</name>
    </author>
    <author>
      <name>Kuai Yu</name>
    </author>
    <author>
      <name>Lean Wang</name>
    </author>
    <author>
      <name>Lecong Zhang</name>
    </author>
    <author>
      <name>Liang Zhao</name>
    </author>
    <author>
      <name>Litong Wang</name>
    </author>
    <author>
      <name>Liyue Zhang</name>
    </author>
    <author>
      <name>Lei Xu</name>
    </author>
    <author>
      <name>Leyi Xia</name>
    </author>
    <author>
      <name>Mingchuan Zhang</name>
    </author>
    <author>
      <name>Minghua Zhang</name>
    </author>
    <author>
      <name>Minghui Tang</name>
    </author>
    <author>
      <name>Meng Li</name>
    </author>
    <author>
      <name>Miaojun Wang</name>
    </author>
    <author>
      <name>Mingming Li</name>
    </author>
    <author>
      <name>Ning Tian</name>
    </author>
    <author>
      <name>Panpan Huang</name>
    </author>
    <author>
      <name>Peng Zhang</name>
    </author>
    <author>
      <name>Qiancheng Wang</name>
    </author>
    <author>
      <name>Qinyu Chen</name>
    </author>
    <author>
      <name>Qiushi Du</name>
    </author>
    <author>
      <name>Ruiqi Ge</name>
    </author>
    <author>
      <name>Ruisong Zhang</name>
    </author>
    <author>
      <name>Ruizhe Pan</name>
    </author>
    <author>
      <name>Runji Wang</name>
    </author>
    <author>
      <name>R. J. Chen</name>
    </author>
    <author>
      <name>R. L. Jin</name>
    </author>
    <author>
      <name>Ruyi Chen</name>
    </author>
    <author>
      <name>Shanghao Lu</name>
    </author>
    <author>
      <name>Shangyan Zhou</name>
    </author>
    <author>
      <name>Shanhuang Chen</name>
    </author>
    <author>
      <name>Shengfeng Ye</name>
    </author>
    <author>
      <name>Shiyu Wang</name>
    </author>
    <author>
      <name>Shuiping Yu</name>
    </author>
    <author>
      <name>Shunfeng Zhou</name>
    </author>
    <author>
      <name>Shuting Pan</name>
    </author>
    <author>
      <name>S. S. Li</name>
    </author>
    <author>
      <name>Shuang Zhou</name>
    </author>
    <author>
      <name>Shaoqing Wu</name>
    </author>
    <author>
      <name>Shengfeng Ye</name>
    </author>
    <author>
      <name>Tao Yun</name>
    </author>
    <author>
      <name>Tian Pei</name>
    </author>
    <author>
      <name>Tianyu Sun</name>
    </author>
    <author>
      <name>T. Wang</name>
    </author>
    <author>
      <name>Wangding Zeng</name>
    </author>
    <author>
      <name>Wanjia Zhao</name>
    </author>
    <author>
      <name>Wen Liu</name>
    </author>
    <author>
      <name>Wenfeng Liang</name>
    </author>
    <author>
      <name>Wenjun Gao</name>
    </author>
    <author>
      <name>Wenqin Yu</name>
    </author>
    <author>
      <name>Wentao Zhang</name>
    </author>
    <author>
      <name>W. L. Xiao</name>
    </author>
    <author>
      <name>Wei An</name>
    </author>
    <author>
      <name>Xiaodong Liu</name>
    </author>
    <author>
      <name>Xiaohan Wang</name>
    </author>
    <author>
      <name>Xiaokang Chen</name>
    </author>
    <author>
      <name>Xiaotao Nie</name>
    </author>
    <author>
      <name>Xin Cheng</name>
    </author>
    <author>
      <name>Xin Liu</name>
    </author>
    <author>
      <name>Xin Xie</name>
    </author>
    <author>
      <name>Xingchao Liu</name>
    </author>
    <author>
      <name>Xinyu Yang</name>
    </author>
    <author>
      <name>Xinyuan Li</name>
    </author>
    <author>
      <name>Xuecheng Su</name>
    </author>
    <author>
      <name>Xuheng Lin</name>
    </author>
    <author>
      <name>X. Q. Li</name>
    </author>
    <author>
      <name>Xiangyue Jin</name>
    </author>
    <author>
      <name>Xiaojin Shen</name>
    </author>
    <author>
      <name>Xiaosha Chen</name>
    </author>
    <author>
      <name>Xiaowen Sun</name>
    </author>
    <author>
      <name>Xiaoxiang Wang</name>
    </author>
    <author>
      <name>Xinnan Song</name>
    </author>
    <author>
      <name>Xinyi Zhou</name>
    </author>
    <author>
      <name>Xianzu Wang</name>
    </author>
    <author>
      <name>Xinxia Shan</name>
    </author>
    <author>
      <name>Y. K. Li</name>
    </author>
    <author>
      <name>Y. Q. Wang</name>
    </author>
    <author>
      <name>Y. X. Wei</name>
    </author>
    <author>
      <name>Yang Zhang</name>
    </author>
    <author>
      <name>Yanhong Xu</name>
    </author>
    <author>
      <name>Yao Li</name>
    </author>
    <author>
      <name>Yao Zhao</name>
    </author>
    <author>
      <name>Yaofeng Sun</name>
    </author>
    <author>
      <name>Yaohui Wang</name>
    </author>
    <author>
      <name>Yi Yu</name>
    </author>
    <author>
      <name>Yichao Zhang</name>
    </author>
    <author>
      <name>Yifan Shi</name>
    </author>
    <author>
      <name>Yiliang Xiong</name>
    </author>
    <author>
      <name>Ying He</name>
    </author>
    <author>
      <name>Yishi Piao</name>
    </author>
    <author>
      <name>Yisong Wang</name>
    </author>
    <author>
      <name>Yixuan Tan</name>
    </author>
    <author>
      <name>Yiyang Ma</name>
    </author>
    <author>
      <name>Yiyuan Liu</name>
    </author>
    <author>
      <name>Yongqiang Guo</name>
    </author>
    <author>
      <name>Yuan Ou</name>
    </author>
    <author>
      <name>Yuduan Wang</name>
    </author>
    <author>
      <name>Yue Gong</name>
    </author>
    <author>
      <name>Yuheng Zou</name>
    </author>
    <author>
      <name>Yujia He</name>
    </author>
    <author>
      <name>Yunfan Xiong</name>
    </author>
    <author>
      <name>Yuxiang Luo</name>
    </author>
    <author>
      <name>Yuxiang You</name>
    </author>
    <author>
      <name>Yuxuan Liu</name>
    </author>
    <author>
      <name>Yuyang Zhou</name>
    </author>
    <author>
      <name>Y. X. Zhu</name>
    </author>
    <author>
      <name>Yanhong Xu</name>
    </author>
    <author>
      <name>Yanping Huang</name>
    </author>
    <author>
      <name>Yaohui Li</name>
    </author>
    <author>
      <name>Yi Zheng</name>
    </author>
    <author>
      <name>Yuchen Zhu</name>
    </author>
    <author>
      <name>Yunxian Ma</name>
    </author>
    <author>
      <name>Ying Tang</name>
    </author>
    <author>
      <name>Yukun Zha</name>
    </author>
    <author>
      <name>Yuting Yan</name>
    </author>
    <author>
      <name>Z. Z. Ren</name>
    </author>
    <author>
      <name>Zehui Ren</name>
    </author>
    <author>
      <name>Zhangli Sha</name>
    </author>
    <author>
      <name>Zhe Fu</name>
    </author>
    <author>
      <name>Zhean Xu</name>
    </author>
    <author>
      <name>Zhenda Xie</name>
    </author>
    <author>
      <name>Zhengyan Zhang</name>
    </author>
    <author>
      <name>Zhewen Hao</name>
    </author>
    <author>
      <name>Zhicheng Ma</name>
    </author>
    <author>
      <name>Zhigang Yan</name>
    </author>
    <author>
      <name>Zhiyu Wu</name>
    </author>
    <author>
      <name>Zihui Gu</name>
    </author>
    <author>
      <name>Zijia Zhu</name>
    </author>
    <author>
      <name>Zijun Liu</name>
    </author>
    <author>
      <name>Zilin Li</name>
    </author>
    <author>
      <name>Ziwei Xie</name>
    </author>
    <author>
      <name>Ziyang Song</name>
    </author>
    <author>
      <name>Zizheng Pan</name>
    </author>
    <author>
      <name>Zhen Huang</name>
    </author>
    <author>
      <name>Zhipeng Xu</name>
    </author>
    <author>
      <name>Zhongyu Zhang</name>
    </author>
    <author>
      <name>Zhen Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12934v1</id>
    <updated>2025-01-22T15:04:13Z</updated>
    <published>2025-01-22T15:04:13Z</published>
    <title>Correctness Assessment of Code Generated by Large Language Models Using
  Internal Representations</title>
    <summary>  Ensuring the correctness of code generated by Large Language Models (LLMs)
presents a significant challenge in AI-driven software development. Existing
approaches predominantly rely on black-box (closed-box) approaches that
evaluate correctness post-generation, failing to utilize the rich insights
embedded in the LLMs' internal states during code generation. In this paper, we
introduce OPENIA, a novel white-box (open-box) framework that leverages these
internal representations to assess the correctness of LLM-generated code.
OPENIA systematically analyzes the intermediate states of representative
open-source LLMs specialized for code, including DeepSeek-Coder, CodeLlama, and
MagicCoder, across diverse code generation benchmarks. Our empirical analysis
reveals that these internal representations encode latent information, which
strongly correlates with the correctness of the generated code. Building on
these insights, OPENIA uses a white-box/open-box approach to make informed
predictions about code correctness, offering significant advantages in
adaptability and robustness over traditional classification-based methods and
zero-shot approaches. Experimental results demonstrate that OPENIA consistently
outperforms baseline models, achieving higher accuracy, precision, recall, and
F1-Scores with up to a 2X improvement in standalone code generation and a 46%
enhancement in repository-specific scenarios. By unlocking the potential of
in-process signals, OPENIA paves the way for more proactive and efficient
quality assurance mechanisms in LLM-assisted code generation.
</summary>
    <author>
      <name>Tuan-Dung Bui</name>
    </author>
    <author>
      <name>Thanh Trong Vu</name>
    </author>
    <author>
      <name>Thu-Trang Nguyen</name>
    </author>
    <author>
      <name>Son Nguyen</name>
    </author>
    <author>
      <name>Hieu Dinh Vo</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12909v1</id>
    <updated>2025-01-22T14:36:30Z</updated>
    <published>2025-01-22T14:36:30Z</published>
    <title>FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in
  Virtual 3D Spaces</title>
    <summary>  Virtual film production requires intricate decision-making processes,
including scriptwriting, virtual cinematography, and precise actor positioning
and actions. Motivated by recent advances in automated decision-making with
language agent-based societies, this paper introduces FilmAgent, a novel
LLM-based multi-agent collaborative framework for end-to-end film automation in
our constructed 3D virtual spaces. FilmAgent simulates various crew roles,
including directors, screenwriters, actors, and cinematographers, and covers
key stages of a film production workflow: (1) idea development transforms
brainstormed ideas into structured story outlines; (2) scriptwriting elaborates
on dialogue and character actions for each scene; (3) cinematography determines
the camera setups for each shot. A team of agents collaborates through
iterative feedback and revisions, thereby verifying intermediate scripts and
reducing hallucinations. We evaluate the generated videos on 15 ideas and 4 key
aspects. Human evaluation shows that FilmAgent outperforms all baselines across
all aspects and scores 3.98 out of 5 on average, showing the feasibility of
multi-agent collaboration in filmmaking. Further analysis reveals that
FilmAgent, despite using the less advanced GPT-4o model, surpasses the
single-agent o1, showing the advantage of a well-coordinated multi-agent
system. Lastly, we discuss the complementary strengths and weaknesses of
OpenAI's text-to-video model Sora and our FilmAgent in filmmaking.
</summary>
    <author>
      <name>Zhenran Xu</name>
    </author>
    <author>
      <name>Longyue Wang</name>
    </author>
    <author>
      <name>Jifang Wang</name>
    </author>
    <author>
      <name>Zhouyi Li</name>
    </author>
    <author>
      <name>Senbao Shi</name>
    </author>
    <author>
      <name>Xue Yang</name>
    </author>
    <author>
      <name>Yiyu Wang</name>
    </author>
    <author>
      <name>Baotian Hu</name>
    </author>
    <author>
      <name>Jun Yu</name>
    </author>
    <author>
      <name>Min Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work in progress. Project Page: https://filmagent.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.12909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12904v1</id>
    <updated>2025-01-22T14:30:40Z</updated>
    <published>2025-01-22T14:30:40Z</published>
    <title>A Functional Software Reference Architecture for LLM-Integrated Systems</title>
    <summary>  The integration of large language models into software systems is
transforming capabilities such as natural language understanding,
decision-making, and autonomous task execution. However, the absence of a
commonly accepted software reference architecture hinders systematic reasoning
about their design and quality attributes. This gap makes it challenging to
address critical concerns like privacy, security, modularity, and
interoperability, which are increasingly important as these systems grow in
complexity and societal impact. In this paper, we describe our
\textit{emerging} results for a preliminary functional reference architecture
as a conceptual framework to address these challenges and guide the design,
evaluation, and evolution of large language model-integrated systems. We
identify key architectural concerns for these systems, informed by current
research and practice. We then evaluate how the architecture addresses these
concerns and validate its applicability using three open-source large language
model-integrated systems in computer vision, text processing, and coding.
</summary>
    <author>
      <name>Alessio Bucaioni</name>
    </author>
    <author>
      <name>Martin Weyssow</name>
    </author>
    <author>
      <name>Junda He</name>
    </author>
    <author>
      <name>Yunbo Lyu</name>
    </author>
    <author>
      <name>David Lo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at the 22nd IEEE International Conference on
  Software Architecture (ICSA 2025) - New and Emerging Ideas</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.12904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12901v1</id>
    <updated>2025-01-22T14:21:04Z</updated>
    <published>2025-01-22T14:21:04Z</published>
    <title>Architectural Fusion Through Contextual Partitioning in Large Language
  Models: A Novel Approach to Parameterized Knowledge Integration</title>
    <summary>  Contextual Partitioning introduces an innovative approach to enhancing the
architectural design of large-scale computational models through the dynamic
segmentation of parameters into context-aware regions. This methodology
emphasizes the importance of task-specific specialization, achieved through
adaptive parameter allocation mechanisms that align with the linguistic
features of input data. Experimental evaluations demonstrated substantial
improvements in accuracy, perplexity, and contextual coherence across a variety
of linguistic tasks, highlighting the adaptability and scalability of the
proposed framework. By reducing redundancy and enhancing computational
efficiency, Contextual Partitioning not only streamlines model operations but
also expands the scope of applications for advanced language processing
systems. The approach operates autonomously, requiring no external fine-tuning,
thereby addressing a significant limitation in conventional parameter
optimization techniques. Empirical results demonstrate the effectiveness of
gradient-driven segmentation, enabling models to dynamically recalibrate and
specialize in response to task-specific demands. Furthermore, resource
utilization metrics reveal notable reductions in memory usage and training
times, confirming the efficiency of the approach. Observations from qualitative
analyses illustrate improved contextual coherence and logical flow in generated
outputs, reinforcing the practical value of this technique. The findings
collectively demonstrate the potential for Contextual Partitioning to redefine
the scalability and adaptability of computational language architectures in
diverse and complex domains.
</summary>
    <author>
      <name>Offa Kingsleigh</name>
    </author>
    <author>
      <name>Alfred Abercrombie</name>
    </author>
    <author>
      <name>David Woolstencroft</name>
    </author>
    <author>
      <name>Beorhtric Meadowcroft</name>
    </author>
    <author>
      <name>Marcus Irvin</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12895v1</id>
    <updated>2025-01-22T14:15:46Z</updated>
    <published>2025-01-22T14:15:46Z</published>
    <title>Test-Time Preference Optimization: On-the-Fly Alignment via Iterative
  Textual Feedback</title>
    <summary>  Large language models (LLMs) demonstrate impressive performance but lack the
flexibility to adapt to human preferences quickly without retraining. In this
work, we introduce Test-time Preference Optimization (TPO), a framework that
aligns LLM outputs with human preferences during inference, removing the need
to update model parameters. Rather than relying on purely numerical rewards,
TPO translates reward signals into textual critiques and uses them as textual
rewards to iteratively refine its response. Evaluations on benchmarks covering
instruction following, preference alignment, safety, and mathematics reveal
that TPO progressively improves alignment with human preferences. Notably,
after only a few TPO steps, the initially unaligned Llama-3.1-70B-SFT model can
surpass the aligned counterpart, Llama-3.1-70B-Instruct. Furthermore, TPO
scales efficiently with both the search width and depth during inference.
Through case studies, we illustrate how TPO exploits the innate capacity of LLM
to interpret and act upon reward signals. Our findings establish TPO as a
practical, lightweight alternative for test-time preference optimization,
achieving alignment on the fly. Our code is publicly available at
https://github.com/yafuly/TPO.
</summary>
    <author>
      <name>Yafu Li</name>
    </author>
    <author>
      <name>Xuyang Hu</name>
    </author>
    <author>
      <name>Xiaoye Qu</name>
    </author>
    <author>
      <name>Linjie Li</name>
    </author>
    <author>
      <name>Yu Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages; work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.12895v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12895v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12883v2</id>
    <updated>2025-01-23T11:12:59Z</updated>
    <published>2025-01-22T13:44:44Z</published>
    <title>Generative AI Misuse Potential in Cyber Security Education: A Case Study
  of a UK Degree Program</title>
    <summary>  Recent advances in generative artificial intelligence (AI), such as ChatGPT,
Google Gemini, and other large language models (LLMs), pose significant
challenges to upholding academic integrity in higher education. This paper
investigates the susceptibility of a Master's-level cyber security degree
program at a UK Russell Group university, accredited by a leading national
body, to LLM misuse. Through the application and extension of a quantitative
assessment framework, we identify a high exposure to misuse, particularly in
independent project- and report-based assessments. Contributing factors,
including block teaching and a predominantly international cohort, are
highlighted as potential amplifiers of these vulnerabilities. To address these
challenges, we discuss the adoption of LLM-resistant assessments, detection
tools, and the importance of fostering an ethical learning environment. These
approaches aim to uphold academic standards while preparing students for the
complexities of real-world cyber security.
</summary>
    <author>
      <name>Carlton Shepherd</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12883v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12883v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12877v1</id>
    <updated>2025-01-22T13:36:46Z</updated>
    <published>2025-01-22T13:36:46Z</published>
    <title>WisdomBot: Tuning Large Language Models with Artificial Intelligence
  Knowledge</title>
    <summary>  Large language models (LLMs) have emerged as powerful tools in natural
language processing (NLP), showing a promising future of artificial generated
intelligence (AGI). Despite their notable performance in the general domain,
LLMs have remained suboptimal in the field of education, owing to the unique
challenges presented by this domain, such as the need for more specialized
knowledge, the requirement for personalized learning experiences, and the
necessity for concise explanations of complex concepts. To address these
issues, this paper presents a novel LLM for education named WisdomBot, which
combines the power of LLMs with educational theories, enabling their seamless
integration into educational contexts. To be specific, we harness
self-instructed knowledge concepts and instructions under the guidance of
Bloom's Taxonomy as training data. To further enhance the accuracy and
professionalism of model's response on factual questions, we introduce two key
enhancements during inference, i.e., local knowledge base retrieval
augmentation and search engine retrieval augmentation during inference. We
substantiate the effectiveness of our approach by applying it to several
Chinese LLMs, thereby showcasing that the fine-tuned models can generate more
reliable and professional responses.
</summary>
    <author>
      <name>Jingyuan Chen</name>
    </author>
    <author>
      <name>Tao Wu</name>
    </author>
    <author>
      <name>Wei Ji</name>
    </author>
    <author>
      <name>Fei Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Frontiers of Digital Education</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.12877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12862v1</id>
    <updated>2025-01-22T13:14:02Z</updated>
    <published>2025-01-22T13:14:02Z</published>
    <title>Mutation-Guided LLM-based Test Generation at Meta</title>
    <summary>  This paper describes Meta's ACH system for mutation-guided LLM-based test
generation. ACH generates relatively few mutants (aka simulated faults),
compared to traditional mutation testing. Instead, it focuses on generating
currently undetected faults that are specific to an issue of concern. From
these currently uncaught faults, ACH generates tests that can catch them,
thereby `killing' the mutants and consequently hardening the platform against
regressions. We use privacy concerns to illustrate our approach, but ACH can
harden code against {\em any} type of regression. In total, ACH was applied to
10,795 Android Kotlin classes in 7 software platforms deployed by Meta, from
which it generated 9,095 mutants and 571 privacy-hardening test cases. ACH also
deploys an LLM-based equivalent mutant detection agent that achieves a
precision of 0.79 and a recall of 0.47 (rising to 0.95 and 0.96 with simple
pre-processing). ACH was used by Messenger and WhatsApp test-a-thons where
engineers accepted 73% of its tests, judging 36% to privacy relevant. We
conclude that ACH hardens code against specific concerns and that, even when
its tests do not directly tackle the specific concern, engineers find them
useful for their other benefits.
</summary>
    <author>
      <name>Christopher Foster</name>
    </author>
    <author>
      <name>Abhishek Gulati</name>
    </author>
    <author>
      <name>Mark Harman</name>
    </author>
    <author>
      <name>Inna Harper</name>
    </author>
    <author>
      <name>Ke Mao</name>
    </author>
    <author>
      <name>Jillian Ritchey</name>
    </author>
    <author>
      <name>Hervé Robert</name>
    </author>
    <author>
      <name>Shubho Sengupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to FSE 2025 Industry Track</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.12862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12851v1</id>
    <updated>2025-01-22T12:59:08Z</updated>
    <published>2025-01-22T12:59:08Z</published>
    <title>ACEBench: Who Wins the Match Point in Tool Learning?</title>
    <summary>  Large language models (LLMs) have demonstrated significant potential in
decision-making and reasoning, especially when combined with various tools to
effectively solve complex problems. However, existing evaluation systems for
assessing LLM function calling capabilities have several limitations: (1)
limited evaluation scenarios, lacking assessments in real multi-turn dialogue
contexts; (2) narrow evaluation dimensions, lacking detailed assessments for
fine-grained function calls; (3) relying on LLMs or real API executions for
result evaluation, which introduces significant overhead. To address these
issues, we propose a comprehensive evaluation system named ACEBench. This
system is meticulously designed to encompass a wide spectrum of function
calling scenarios. Moreover, it categorizes these scenarios into three primary
types according to the evaluation methodology: Normal, Special, and Agent.
Normal evaluates function calls in basic scenarios; Special evaluates function
calls in scenarios with vague or incomplete instructions; Agent introduces
multi-agent interactions to simulate function calling evaluation in real-world
multi-turn interactions. We conducted extensive experiments on ACEBench,
analyzing various LLMs in-depth and performing a more granular analysis of
error causes across different data types.
</summary>
    <author>
      <name>Chen Chen</name>
    </author>
    <author>
      <name>Xinlong Hao</name>
    </author>
    <author>
      <name>Weiwen Liu</name>
    </author>
    <author>
      <name>Xu Huang</name>
    </author>
    <author>
      <name>Xingshan Zeng</name>
    </author>
    <author>
      <name>Shuai Yu</name>
    </author>
    <author>
      <name>Dexun Li</name>
    </author>
    <author>
      <name>Shuai Wang</name>
    </author>
    <author>
      <name>Weinan Gan</name>
    </author>
    <author>
      <name>Yuefeng Huang</name>
    </author>
    <author>
      <name>Xinzhi Wang</name>
    </author>
    <author>
      <name>Defu Lian</name>
    </author>
    <author>
      <name>Baoqun Yin</name>
    </author>
    <author>
      <name>Yasheng Wang</name>
    </author>
    <author>
      <name>Wu Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12835v1</id>
    <updated>2025-01-22T12:21:17Z</updated>
    <published>2025-01-22T12:21:17Z</published>
    <title>Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back
  Home</title>
    <summary>  Retrieval Augmented Generation (RAG) improves correctness of Question
Answering (QA) and addresses hallucinations in Large Language Models (LLMs),
yet greatly increase computational costs. Besides, RAG is not always needed as
may introduce irrelevant information. Recent adaptive retrieval methods
integrate LLMs' intrinsic knowledge with external information appealing to LLM
self-knowledge, but they often neglect efficiency evaluations and comparisons
with uncertainty estimation techniques. We bridge this gap by conducting a
comprehensive analysis of 35 adaptive retrieval methods, including 8 recent
approaches and 27 uncertainty estimation techniques, across 6 datasets using 10
metrics for QA performance, self-knowledge, and efficiency. Our findings show
that uncertainty estimation techniques often outperform complex pipelines in
terms of efficiency and self-knowledge, while maintaining comparable QA
performance.
</summary>
    <author>
      <name>Viktor Moskvoretskii</name>
    </author>
    <author>
      <name>Maria Lysyuk</name>
    </author>
    <author>
      <name>Mikhail Salnikov</name>
    </author>
    <author>
      <name>Nikolay Ivanov</name>
    </author>
    <author>
      <name>Sergey Pletenev</name>
    </author>
    <author>
      <name>Daria Galimzianova</name>
    </author>
    <author>
      <name>Nikita Krayko</name>
    </author>
    <author>
      <name>Vasily Konovalov</name>
    </author>
    <author>
      <name>Irina Nikishina</name>
    </author>
    <author>
      <name>Alexander Panchenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The code and data will be published soon</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.12835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12826v1</id>
    <updated>2025-01-22T12:06:16Z</updated>
    <published>2025-01-22T12:06:16Z</published>
    <title>Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek</title>
    <summary>  Natural Language Processing (NLP) for lesser-resourced languages faces
persistent challenges, including limited datasets, inherited biases from
high-resource languages, and the need for domain-specific solutions. This study
addresses these gaps for Modern Greek through three key contributions. First,
we evaluate the performance of open-source (Llama-70b) and closed-source
(GPT-4o mini) large language models (LLMs) on seven core NLP tasks with dataset
availability, revealing task-specific strengths, weaknesses, and parity in
their performance. Second, we expand the scope of Greek NLP by reframing
Authorship Attribution as a tool to assess potential data usage by LLMs in
pre-training, with high 0-shot accuracy suggesting ethical implications for
data provenance. Third, we showcase a legal NLP case study, where a Summarize,
Translate, and Embed (STE) methodology outperforms the traditional TF-IDF
approach for clustering \emph{long} legal texts. Together, these contributions
provide a roadmap to advance NLP in lesser-resourced languages, bridging gaps
in model evaluation, task innovation, and real-world impact.
</summary>
    <author>
      <name>John Pavlopoulos</name>
    </author>
    <author>
      <name>Juli Bakagianni</name>
    </author>
    <author>
      <name>Kanella Pouli</name>
    </author>
    <author>
      <name>Maria Gavriilidou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NLP, Modern Greek, benchmark, machine learning, language resources</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.12826v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12826v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12793v1</id>
    <updated>2025-01-22T10:54:19Z</updated>
    <published>2025-01-22T10:54:19Z</published>
    <title>Revisit Self-Debugging with Self-Generated Tests for Code Generation</title>
    <summary>  Large language models (LLMs) have shown significant advancements in code
generation, but still face challenges on tasks beyond their basic capabilities.
Recently, the notion of self-debugging has been proposed to boost the
performance of code generation by leveraging execution feedback from tests.
Despite its promise, the availability of high-quality tests in real-world
scenarios is limited. In this context, self-debugging with self-generated tests
is a promising solution but lacks a full exploration of its limitations and
practical potential. Therefore, we investigate its efficacy on diverse
programming problems. To deepen our understanding, we propose two distinct
paradigms for the process: post-execution and in-execution self-debugging.
Within the scope of self-contained Python programming tasks, we find that
post-execution self-debugging struggles on basic problems but shows potential
for improvement on competitive ones, due to the bias introduced by
self-generated tests. On the other hand, in-execution self-debugging enables
LLMs to mitigate the bias by solely leveraging intermediate states during
execution, thereby enhancing code generation.
</summary>
    <author>
      <name>Xiancai Chen</name>
    </author>
    <author>
      <name>Zhengwei Tao</name>
    </author>
    <author>
      <name>Kechi Zhang</name>
    </author>
    <author>
      <name>Changzhi Zhou</name>
    </author>
    <author>
      <name>Wanli Gu</name>
    </author>
    <author>
      <name>Yuanpeng He</name>
    </author>
    <author>
      <name>Mengdi Zhang</name>
    </author>
    <author>
      <name>Xunliang Cai</name>
    </author>
    <author>
      <name>Haiyan Zhao</name>
    </author>
    <author>
      <name>Zhi Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work in Progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.12793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12789v1</id>
    <updated>2025-01-22T10:47:08Z</updated>
    <published>2025-01-22T10:47:08Z</published>
    <title>Generating Diverse Q&amp;A Benchmarks for RAG Evaluation with DataMorgana</title>
    <summary>  Evaluating Retrieval-Augmented Generation (RAG) systems, especially in
domain-specific contexts, requires benchmarks that address the distinctive
requirements of the applicative scenario. Since real data can be hard to
obtain, a common strategy is to use LLM-based methods to generate synthetic
data. Existing solutions are general purpose: given a document, they generate a
question to build a Q&amp;A pair. However, although the generated questions can be
individually good, they are typically not diverse enough to reasonably cover
the different ways real end-users can interact with the RAG system. We
introduce here DataMorgana, a tool for generating highly customizable and
diverse synthetic Q&amp;A benchmarks tailored to RAG applications. DataMorgana
enables detailed configurations of user and question categories and provides
control over their distribution within the benchmark. It uses a lightweight
two-stage process, ensuring efficiency and fast iterations, while generating
benchmarks that reflect the expected traffic. We conduct a thorough line of
experiments, showing quantitatively and qualitatively that DataMorgana
surpasses existing tools and approaches in producing lexically, syntactically,
and semantically diverse question sets across domain-specific and
general-knowledge corpora. DataMorgana will be made available to selected teams
in the research community, as first beta testers, in the context of the
upcoming SIGIR'2025 LiveRAG challenge to be announced in early February 2025.
</summary>
    <author>
      <name>Simone Filice</name>
    </author>
    <author>
      <name>Guy Horowitz</name>
    </author>
    <author>
      <name>David Carmel</name>
    </author>
    <author>
      <name>Zohar Karnin</name>
    </author>
    <author>
      <name>Liane Lewin-Eytan</name>
    </author>
    <author>
      <name>Yoelle Maarek</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12774v1</id>
    <updated>2025-01-22T10:16:53Z</updated>
    <published>2025-01-22T10:16:53Z</published>
    <title>LLMs as Repositories of Factual Knowledge: Limitations and Solutions</title>
    <summary>  LLMs' sources of knowledge are data snapshots containing factual information
about entities collected at different timestamps and from different media types
(e.g. wikis, social media, etc.). Such unstructured knowledge is subject to
change due to updates through time from past to present. Equally important are
the inconsistencies and inaccuracies occurring in different information
sources. Consequently, the model's knowledge about an entity may be perturbed
while training over the sequence of snapshots or at inference time, resulting
in inconsistent and inaccurate model performance. In this work, we study the
appropriateness of Large Language Models (LLMs) as repositories of factual
knowledge. We consider twenty-four state-of-the-art LLMs that are either
closed-, partially (weights), or fully (weight and training data) open-source.
We evaluate their reliability in responding to time-sensitive factual questions
in terms of accuracy and consistency when prompts are perturbed. We further
evaluate the effectiveness of state-of-the-art methods to improve LLMs'
accuracy and consistency. We then propose "ENtity-Aware Fine-tuning" (ENAF), a
soft neurosymbolic approach aimed at providing a structured representation of
entities during fine-tuning to improve the model's performance.
</summary>
    <author>
      <name>Seyed Mahed Mousavi</name>
    </author>
    <author>
      <name>Simone Alghisi</name>
    </author>
    <author>
      <name>Giuseppe Riccardi</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12766v1</id>
    <updated>2025-01-22T10:01:54Z</updated>
    <published>2025-01-22T10:01:54Z</published>
    <title>NExtLong: Toward Effective Long-Context Training without Long Documents</title>
    <summary>  Large language models (LLMs) with extended context windows have made
significant strides yet remain a challenge due to the scarcity of long
documents. Existing methods tend to synthesize long-context data but lack a
clear mechanism to reinforce the long-range dependency modeling. To address
this limitation, we propose NExtLong, a novel framework for synthesizing
long-context data through Negative document Extension. NExtLong decomposes a
document into multiple meta-chunks and extends the context by interleaving hard
negative distractors retrieved from pretraining corpora. This approach compels
the model to discriminate long-range dependent context from distracting
content, enhancing its ability to model long-range dependencies. Extensive
experiments demonstrate that NExtLong achieves significant performance
improvements on the HELMET and RULER benchmarks compared to existing
long-context synthesis approaches and leading models, which are trained on
non-synthetic long documents. These findings highlight NExtLong's ability to
reduce reliance on non-synthetic long documents, making it an effective
framework for developing advanced long-context LLMs.
</summary>
    <author>
      <name>Chaochen Gao</name>
    </author>
    <author>
      <name>Xing Wu</name>
    </author>
    <author>
      <name>Zijia Lin</name>
    </author>
    <author>
      <name>Debing Zhang</name>
    </author>
    <author>
      <name>Songlin Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corresponding authors: xing wu, and songlin hu</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.12766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13723v1</id>
    <updated>2025-01-22T09:58:26Z</updated>
    <published>2025-01-22T09:58:26Z</published>
    <title>Intelligent Exercise and Feedback System for Social Healthcare using
  LLMOps</title>
    <summary>  This study addresses the growing demand for personalized feedback in
healthcare platforms and social communities by introducing an LLMOps-based
system for automated exercise analysis and personalized recommendations.
Current healthcare platforms rely heavily on manual analysis and generic health
advice, limiting user engagement and health promotion effectiveness. We
developed a system that leverages Large Language Models (LLM) to automatically
analyze user activity data from the "Ounwan" exercise recording community. The
system integrates LLMOps with LLM APIs, containerized infrastructure, and CI/CD
practices to efficiently process large-scale user activity data, identify
patterns, and generate personalized recommendations. The architecture ensures
scalability, reliability, and security for large-scale healthcare communities.
Evaluation results demonstrate the system's effectiveness in three key metrics:
exercise classification, duration prediction, and caloric expenditure
estimation. This approach improves the efficiency of community management while
providing more accurate and personalized feedback to users, addressing the
limitations of traditional manual analysis methods.
</summary>
    <author>
      <name>Yeongrak Choi</name>
    </author>
    <author>
      <name>Taeyoung Kim</name>
    </author>
    <author>
      <name>Hyung Soo Han</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12746v1</id>
    <updated>2025-01-22T09:27:11Z</updated>
    <published>2025-01-22T09:27:11Z</published>
    <title>EvidenceMap: Unleashing the Power of Small Language Models with Evidence
  Analysis for Biomedical Question Answering</title>
    <summary>  Current LLM-based approaches improve question answering performance by
leveraging the internal reasoning abilities of models or incorporating external
knowledge. However, when humans address professional problems, it is essential
to explicitly analyze the multifaceted relationships from multiple pieces and
diverse sources of evidence to achieve better answers. In this study, we
propose a novel generative question answering framework for the biomedical
domain, named EvidenceMap, which explicitly learns and incorporates evidence
analysis with small language models (SLMs). The framework describes an evidence
map for each question and fully utilizes an SLM to derive the representation of
the supportive evaluation, the logical correlation, and the summarization of
the related evidence, which facilitates an analysis-augmented generation with
another SLM in an autoregressive way. Extensive experiments have shown that
introducing an evidence analysis learning process can significantly outperform
larger models and popular LLM reasoning methods.
</summary>
    <author>
      <name>Chang Zong</name>
    </author>
    <author>
      <name>Jian Wan</name>
    </author>
    <author>
      <name>Lei Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.12746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12735v1</id>
    <updated>2025-01-22T09:12:09Z</updated>
    <published>2025-01-22T09:12:09Z</published>
    <title>Online Preference Alignment for Language Models via Count-based
  Exploration</title>
    <summary>  Reinforcement Learning from Human Feedback (RLHF) has shown great potential
in fine-tuning Large Language Models (LLMs) to align with human preferences.
Existing methods perform preference alignment from a fixed dataset, which can
be limited in data coverage, and the resulting reward model is hard to
generalize in out-of-distribution responses. Thus, online RLHF is more
desirable to empower the LLM to explore outside the support of the initial
dataset by iteratively collecting the prompt-response pairs. In this paper, we
study the fundamental problem in online RLHF, i.e. \emph{how to explore} for
LLM. We give a theoretical motivation in linear reward assumption to show that
an optimistic reward with an upper confidence bound (UCB) term leads to a
provably efficient RLHF policy. Then, we reformulate our objective to direct
preference optimization with an exploration term, where the UCB-term can be
converted to a count-based exploration bonus. We further propose a practical
algorithm, named \emph{Count-based Online Preference Optimization (COPO)},
which leverages a simple coin-flip counting module to estimate the pseudo-count
of a prompt-response pair in previously collected data. COPO encourages LLMs to
balance exploration and preference optimization in an iterative manner, which
enlarges the exploration space and the entire data coverage of iterative LLM
policies. We conduct online RLHF experiments on Zephyr and Llama-3 models. The
results on instruction-following and standard academic benchmarks show that
COPO significantly increases performance.
</summary>
    <author>
      <name>Chenjia Bai</name>
    </author>
    <author>
      <name>Yang Zhang</name>
    </author>
    <author>
      <name>Shuang Qiu</name>
    </author>
    <author>
      <name>Qiaosheng Zhang</name>
    </author>
    <author>
      <name>Kang Xu</name>
    </author>
    <author>
      <name>Xuelong Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICLR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.12735v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12735v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12728v1</id>
    <updated>2025-01-22T09:05:01Z</updated>
    <published>2025-01-22T09:05:01Z</published>
    <title>A Call for Critically Rethinking and Reforming Data Analysis in
  Empirical Software Engineering</title>
    <summary>  Context: Empirical Software Engineering (ESE) drives innovation in SE through
qualitative and quantitative studies. However, concerns about the correct
application of empirical methodologies have existed since the 2006 Dagstuhl
seminar on SE. Objective: To analyze three decades of SE research, identify
mistakes in statistical methods, and evaluate experts' ability to detect and
address these issues. Methods: We conducted a literature survey of ~27,000
empirical studies, using LLMs to classify statistical methodologies as adequate
or inadequate. Additionally, we selected 30 primary studies and held a workshop
with 33 ESE experts to assess their ability to identify and resolve statistical
issues. Results: Significant statistical issues were found in the primary
studies, and experts showed limited ability to detect and correct these
methodological problems, raising concerns about the broader ESE community's
proficiency in this area. Conclusions. Despite our study's eventual
limitations, its results shed light on recurring issues from promoting
information copy-and-paste from past authors' works and the continuous
publication of inadequate approaches that promote dubious results and
jeopardize the spread of the correct statistical strategies among researchers.
Besides, it justifies further investigation into empirical rigor in software
engineering to expose these recurring issues and establish a framework for
reassessing our field's foundation of statistical methodology application.
Therefore, this work calls for critically rethinking and reforming data
analysis in empirical software engineering, paving the way for our work soon.
</summary>
    <author>
      <name>Matteo Esposito</name>
    </author>
    <author>
      <name>Mikel Robredo</name>
    </author>
    <author>
      <name>Murali Sridharan</name>
    </author>
    <author>
      <name>Guilherme Horta Travassos</name>
    </author>
    <author>
      <name>Rafael Peñaloza</name>
    </author>
    <author>
      <name>Valentina Lenarduzzi</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12702v1</id>
    <updated>2025-01-22T08:18:37Z</updated>
    <published>2025-01-22T08:18:37Z</published>
    <title>Paradigm-Based Automatic HDL Code Generation Using LLMs</title>
    <summary>  While large language models (LLMs) have demonstrated the ability to generate
hardware description language (HDL) code for digital circuits, they still face
the hallucination problem, which can result in the generation of incorrect HDL
code or misinterpretation of specifications. In this work, we introduce a
human-expert-inspired method to mitigate the hallucination of LLMs and enhance
their performance in HDL code generation. We begin by constructing specialized
paradigm blocks that consist of several steps designed to divide and conquer
generation tasks, mirroring the design methodology of human experts. These
steps include information extraction, human-like design flows, and the
integration of external tools. LLMs are then instructed to classify the type of
circuit in order to match it with the appropriate paradigm block and execute
the block to generate the HDL codes. Additionally, we propose a two-phase
workflow for multi-round generation, aimed at effectively improving the
testbench pass rate of the generated HDL codes within a limited number of
generation and verification rounds. Experimental results demonstrate that our
method significantly enhances the functional correctness of the generated
Verilog code
</summary>
    <author>
      <name>Wenhao Sun</name>
    </author>
    <author>
      <name>Bing Li</name>
    </author>
    <author>
      <name>Grace Li Zhang</name>
    </author>
    <author>
      <name>Xunzhao Yin</name>
    </author>
    <author>
      <name>Cheng Zhuo</name>
    </author>
    <author>
      <name>Ulf Schlichtmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by ISQED2025. arXiv admin note: text overlap with
  arXiv:2407.18326</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.12702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12698v1</id>
    <updated>2025-01-22T08:14:51Z</updated>
    <published>2025-01-22T08:14:51Z</published>
    <title>Training Dialogue Systems by AI Feedback for Improving Overall Dialogue
  Impression</title>
    <summary>  To improve user engagement during conversations with dialogue systems, we
must improve individual dialogue responses and dialogue impressions such as
consistency, personality, and empathy throughout the entire dialogue. While
such dialogue systems have been developing rapidly with the help of large
language models (LLMs), reinforcement learning from AI feedback (RLAIF) has
attracted attention to align LLM-based dialogue models for such dialogue
impressions. In RLAIF, a reward model based on another LLM is used to create a
training signal for an LLM-based dialogue model using zero-shot/few-shot
prompting techniques. However, evaluating an entire dialogue only by prompting
LLMs is challenging. In this study, the supervised fine-tuning (SFT) of LLMs
prepared reward models corresponding to 12 metrics related to the impression of
the entire dialogue for evaluating dialogue responses. We tuned our dialogue
models using the reward model signals as feedback to improve the impression of
the system. The results of automatic and human evaluations showed that tuning
the dialogue model using our reward model corresponding to dialogue impression
improved the evaluation of individual metrics and the naturalness of the
dialogue response.
</summary>
    <author>
      <name>Kai Yoshida</name>
    </author>
    <author>
      <name>Masahiro Mizukami</name>
    </author>
    <author>
      <name>Seiya Kawano</name>
    </author>
    <author>
      <name>Canasai Kruengkrai</name>
    </author>
    <author>
      <name>Hiroaki Sugiyama</name>
    </author>
    <author>
      <name>Koichiro Yoshino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICASSP 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.12698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12697v1</id>
    <updated>2025-01-22T08:14:11Z</updated>
    <published>2025-01-22T08:14:11Z</published>
    <title>Combining Knowledge Graph and LLMs for Enhanced Zero-shot Visual
  Question Answering</title>
    <summary>  Zero-shot visual question answering (ZS-VQA), an emerged critical research
area, intends to answer visual questions without providing training samples.
Existing research in ZS-VQA has proposed to leverage knowledge graphs or large
language models (LLMs), respectively, as external information sources to help
VQA model comprehend images and questions. However, LLMs often struggle in
accurately interpreting specific question meanings. Meanwhile, although
knowledge graph has rich entity relationships, it is challenging to effectively
connect entities to individual image content for visual question answers. In
this paper, we propose a novel design to combine knowledge graph and LLMs for
zero-shot visual question answer. Our approach uses LLMs' powerful
understanding capabilities to accurately interpret image content through a
strategic question search mechanism. Meanwhile, the knowledge graph is used to
expand and connect users' queries to the image content for better visual
question answering. An optimization algorithm is further used to determine the
optimal weights for the loss functions derived from different information
sources, towards a globally optimal set of candidate answers. Experimental
results on two benchmark datasets demonstrate that our model achieves
state-of-the-art (SOTA) performance. Both source code and benchmark data will
be released for public access.
</summary>
    <author>
      <name>Qian Tao</name>
    </author>
    <author>
      <name>Xiaoyang Fan</name>
    </author>
    <author>
      <name>Yong Xu</name>
    </author>
    <author>
      <name>Xingquan Zhu</name>
    </author>
    <author>
      <name>Yufei Tang</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12689v1</id>
    <updated>2025-01-22T07:52:38Z</updated>
    <published>2025-01-22T07:52:38Z</published>
    <title>EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation</title>
    <summary>  Large language models (LLMs) have excelled in various applications, yet
serving them at scale is challenging due to their substantial resource demands
and high latency. Our real-world studies reveal that over 60% of user requests
to LLMs have semantically similar counterparts, suggesting the potential for
knowledge sharing among requests. However, naively caching and reusing past
responses leads to large quality degradation. In this paper, we introduce
EchoLM, an in-context caching system that leverages historical requests as
examples to guide response generation, enabling selective offloading of
requests to more efficient LLMs. However, enabling this real-time knowledge
transfer leads to intricate tradeoffs between response quality, latency, and
system throughput at scale. For a new request, EchoLM identifies similar,
high-utility examples and efficiently prepends them to the input for better
response. At scale, EchoLM adaptively routes requests to LLMs of varying
capabilities, accounting for response quality and serving loads. EchoLM employs
a cost-aware cache replay mechanism to improve example quality and coverage
offline, maximizing cache utility and runtime efficiency. Evaluations on
millions of open-source requests demonstrate that EchoLM has a throughput
improvement of 1.4-5.9x while reducing latency by 28-71% without hurting
response quality on average.
</summary>
    <author>
      <name>Yifan Yu</name>
    </author>
    <author>
      <name>Yu Gan</name>
    </author>
    <author>
      <name>Lily Tasi</name>
    </author>
    <author>
      <name>Nikhil Sarda</name>
    </author>
    <author>
      <name>Jiaming Shen</name>
    </author>
    <author>
      <name>Yanqi Zhou</name>
    </author>
    <author>
      <name>Arvind Krishnamurthy</name>
    </author>
    <author>
      <name>Fan Lai</name>
    </author>
    <author>
      <name>Henry M. Levy</name>
    </author>
    <author>
      <name>David Culler</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12642v1</id>
    <updated>2025-01-22T05:03:51Z</updated>
    <published>2025-01-22T05:03:51Z</published>
    <title>Training Data Attribution (TDA): Examining Its Adoption &amp; Use Cases</title>
    <summary>  This report investigates Training Data Attribution (TDA) and its potential
importance to and tractability for reducing extreme risks from AI. First, we
discuss the plausibility and amount of effort it would take to bring existing
TDA research efforts from their current state, to an efficient and accurate
tool for TDA inference that can be run on frontier-scale LLMs. Next, we discuss
the numerous research benefits AI labs will expect to see from using such TDA
tooling. Then, we discuss a key outstanding bottleneck that would limit such
TDA tooling from being accessible publicly: AI labs' willingness to disclose
their training data. We suggest ways AI labs may work around these limitations,
and discuss the willingness of governments to mandate such access. Assuming
that AI labs willingly provide access to TDA inference, we then discuss what
high-level societal benefits you might see. We list and discuss a series of
policies and systems that may be enabled by TDA. Finally, we present an
evaluation of TDA's potential impact on mitigating large-scale risks from AI
systems.
</summary>
    <author>
      <name>Deric Cheng</name>
    </author>
    <author>
      <name>Juhan Bae</name>
    </author>
    <author>
      <name>Justin Bullock</name>
    </author>
    <author>
      <name>David Kristofferson</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12634v1</id>
    <updated>2025-01-22T04:42:19Z</updated>
    <published>2025-01-22T04:42:19Z</published>
    <title>SoMa: Identifying, Exploring, and Understanding the DRAM Communication
  Scheduling Space for DNN Accelerators</title>
    <summary>  Modern Deep Neural Network (DNN) accelerators are equipped with increasingly
larger on-chip buffers to provide more opportunities to alleviate the
increasingly severe DRAM bandwidth pressure. However, most existing research on
buffer utilization still primarily focuses on single-layer dataflow scheduling
optimization. As buffers grow large enough to accommodate most single-layer
weights in most networks, the impact of single-layer dataflow optimization on
DRAM communication diminishes significantly. Therefore, developing new
paradigms that fuse multiple layers to fully leverage the increasingly abundant
on-chip buffer resources to reduce DRAM accesses has become particularly
important, yet remains an open challenge. To address this challenge, we first
identify the optimization opportunities in DRAM communication scheduling by
analyzing the drawbacks of existing works on the layer fusion paradigm and
recognizing the vast optimization potential in scheduling the timing of data
prefetching from and storing to DRAM. To fully exploit these optimization
opportunities, we develop a Tensor-centric Notation and its corresponding
parsing method to represent different DRAM communication scheduling schemes and
depict the overall space of DRAM communication scheduling. Then, to thoroughly
and efficiently explore the space of DRAM communication scheduling for diverse
accelerators and workloads, we develop an end-to-end scheduling framework,
SoMa, which has already been developed into a compiler for our commercial
accelerator product. Compared with the state-of-the-art (SOTA) Cocco framework,
SoMa achieves, on average, a 2.11x performance improvement and a 37.3%
reduction in energy cost simultaneously. Then, we leverage SoMa to study
optimizations for LLM, perform design space exploration (DSE), and analyze the
DRAM communication scheduling space through a practical example, yielding
some..(more)
</summary>
    <author>
      <name>Jingwei Cai</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tsinghua University</arxiv:affiliation>
    </author>
    <author>
      <name>Xuan Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Xi'an Jiaotong University</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IIISCT</arxiv:affiliation>
    </author>
    <author>
      <name>Mingyu Gao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tsinghua University</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shanghai AI Laboratory</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shanghai Qi Zhi Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Sen Peng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Xi'an Jiaotong University</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IIISCT</arxiv:affiliation>
    </author>
    <author>
      <name>Zijian Zhu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tsinghua University</arxiv:affiliation>
    </author>
    <author>
      <name>Yuchen Wei</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tsinghua University</arxiv:affiliation>
    </author>
    <author>
      <name>Zuotong Wu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Xi'an Jiaotong University</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IIISCT</arxiv:affiliation>
    </author>
    <author>
      <name>Kaisheng Ma</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tsinghua University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by 2025 IEEE International Symposium on High-Performance
  Computer Architecture (HPCA)</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.12634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12619v1</id>
    <updated>2025-01-22T03:57:52Z</updated>
    <published>2025-01-22T03:57:52Z</published>
    <title>Distillation Quantification for Large Language Models</title>
    <summary>  Model distillation is a technique for transferring knowledge from large
language models (LLMs) to smaller ones, aiming to create resource-efficient yet
high-performing models. However, excessive distillation can lead to
homogenization, reducing diversity among models and impairing their ability to
robustly handle complex or novel tasks. These limitations underscore the need
to systematically quantify the distillation process and its impact. In this
work, we propose a framework to evaluate and quantify model distillation. Our
method addresses two key aspects: (1) Identifying identity cognition
contradictions to assess discrepancies in how models perceive and represent
identity-related information, and (2) Analyzing multi-granularity response
similarities across models to measure the extent of homogenization.
Experimental results demonstrate two key insights: (1) Well-known closed-source
and open-source LLMs usually exhibit high distillation degrees, except for
Claude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees
compared to aligned LLMs. By offering a systematic approach to improve the
transparency of LLM data distillation, we call for LLMs with more independent
development and more transparent technical reports to improve LLMs' robustness
and safety. The code and data are available under
https://github.com/Aegis1863/LLMs-Distillation-Quantification.
</summary>
    <author>
      <name>Sunbowen Lee</name>
    </author>
    <author>
      <name>Junting Zhou</name>
    </author>
    <author>
      <name>Chang Ao</name>
    </author>
    <author>
      <name>Kaige Li</name>
    </author>
    <author>
      <name>Xinrun Du</name>
    </author>
    <author>
      <name>Sirui He</name>
    </author>
    <author>
      <name>Jiaheng Liu</name>
    </author>
    <author>
      <name>Min Yang</name>
    </author>
    <author>
      <name>Zhoufutu Wen</name>
    </author>
    <author>
      <name>Shiwen Ni</name>
    </author>
    <link href="http://arxiv.org/abs/2501.12619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.12617v1</id>
    <updated>2025-01-22T03:51:56Z</updated>
    <published>2025-01-22T03:51:56Z</published>
    <title>Deep Learning-Based Identification of Inconsistent Method Names: How Far
  Are We?</title>
    <summary>  Concise and meaningful method names are crucial for program comprehension and
maintenance. However, method names may become inconsistent with their
corresponding implementations, causing confusion and errors. Several deep
learning (DL)-based approaches have been proposed to identify such
inconsistencies, with initial evaluations showing promising results. However,
these evaluations typically use a balanced dataset, where the number of
inconsistent and consistent names are equal. This setup, along with flawed
dataset construction, leads to false positives, making reported performance
less reliable in real-world scenarios, where most method names are consistent.
In this paper, we present an empirical study that evaluates state-of-the-art
DL-based methods for identifying inconsistent method names. We create a new
benchmark by combining automatic identification from commit histories and
manual developer inspections, reducing false positives. We evaluate five
representative DL approaches (one retrieval-based and four generation-based) on
this benchmark. Our results show that performance drops substantially when
moving from the balanced dataset to the new benchmark. We further conduct
quantitative and qualitative analyses to understand the strengths and
weaknesses of the approaches. Retrieval-based methods perform well on simple
methods and those with popular name sub-tokens but fail due to inefficient
representation techniques. Generation-based methods struggle with inaccurate
similarity calculations and immature name generation. Based on these findings,
we propose improvements using contrastive learning and large language models
(LLMs). Our study suggests that significant improvements are needed before
these DL approaches can be effectively applied to real-world software systems.
</summary>
    <author>
      <name>Taiming Wang</name>
    </author>
    <author>
      <name>Yuxia Zhang</name>
    </author>
    <author>
      <name>Lin Jiang</name>
    </author>
    <author>
      <name>Yi Tang</name>
    </author>
    <author>
      <name>Guangjie Li</name>
    </author>
    <author>
      <name>Hui Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10664-024-10592-z</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10664-024-10592-z" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Empirical Software Engineering, 2025, 30(1): 31</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2501.12617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.12617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
